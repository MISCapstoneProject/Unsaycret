#!/usr/bin/env python3
"""
Dataset analysis utility for fixed-window two-speaker speech separation metadata.

The script ingests the metadata CSV files generated by build_dataset.py, performs
aggregate statistics, sanity validations, deviation checks against expected
targets, and optionally produces diagnostic plots.
"""

from __future__ import annotations

import argparse
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

# Global constants
CLIP_DURATION_SECONDS = 4.0
SPLITS = ("train", "valid", "test")
DEFAULT_EXPECT_TWO_SPK = 0.45
DEFAULT_EXPECT_COVERAGE = {
    0.25: 0.25,
    0.50: 0.25,
    0.75: 0.25,
    1.0: 0.25,
}
DEFAULT_EXPECT_OVERLAP = {
    "0": 0.25,
    "(0,20]": 0.35,
    "(20,50]": 0.30,
    "(50,100]": 0.10,
}
RATIO_TOL = 1e-3
FOUR_SEC_TOL = 4.001


@dataclass
class SummaryOutputs:
    """Container for tabular summaries."""

    summary_global: pd.DataFrame
    summary_by_split: pd.DataFrame
    coverage_by_split: pd.DataFrame
    placement_by_split: pd.DataFrame
    overlap_bucket_by_split: pd.DataFrame
    overlap_stats: pd.DataFrame
    gender_counts: pd.DataFrame
    institute_counts: pd.DataFrame


def parse_args() -> argparse.Namespace:
    """CLI argument parsing."""
    parser = argparse.ArgumentParser(description="Analyze speech separation dataset metadata.")
    parser.add_argument(
        "--metadata_dir",
        required=True,
        type=str,
        help="Directory containing train.csv, valid.csv, test.csv metadata files.",
    )
    parser.add_argument(
        "--out_dir",
        required=True,
        type=str,
        help="Directory to write analysis outputs.",
    )
    parser.add_argument(
        "--expect_two_spk_ratio",
        type=float,
        default=DEFAULT_EXPECT_TWO_SPK,
        help="Expected fraction of two-speaker clips (default 0.45).",
    )
    parser.add_argument(
        "--expect_coverage",
        type=str,
        default=None,
        help="Expected coverage ratios as value:prob pairs. Separate entries by comma or semicolon. "
        "Defaults to uniform distribution.",
    )
    parser.add_argument(
        "--expect_overlap",
        type=str,
        default=None,
        help="Expected overlap bucket ratios for two-speaker clips. Supports comma/semicolon separators.",
    )
    parser.add_argument(
        "--plots",
        action="store_true",
        help="Generate matplotlib plots saved to the output directory.",
    )
    return parser.parse_args()


def load_csvs(metadata_dir: Path) -> pd.DataFrame:
    """
    Load metadata CSVs for all splits and concatenate them, adding a 'split' column.

    Parameters
    ----------
    metadata_dir : Path
        Directory containing train.csv, valid.csv, and test.csv.

    Returns
    -------
    pd.DataFrame
        Combined dataframe with an added 'split' column.
    """
    frames: List[pd.DataFrame] = []
    for split in SPLITS:
        csv_path = metadata_dir / f"{split}.csv"
        if not csv_path.exists():
            raise FileNotFoundError(f"Missing metadata file: {csv_path}")
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
        df["split"] = split
        frames.append(df)
    combined = pd.concat(frames, ignore_index=True)
    numeric_cols = [
        "num_speakers",
        "coverage_ratio",
        "snr_db",
        "silence_floor_dbfs",
        "sample_rate",
        "s1_start",
        "s1_dur",
        "s2_start",
        "s2_dur",
        "overlap_sec",
        "overlap_ratio_window",
    ]
    for col in numeric_cols:
        if col in combined.columns:
            combined[col] = pd.to_numeric(combined[col], errors="coerce")
    if "num_speakers" not in combined.columns:
        raise KeyError("Column 'num_speakers' is required in metadata CSVs.")
    if combined["num_speakers"].isna().any():
        bad_ids = combined.loc[combined["num_speakers"].isna(), "id"].tolist()
        raise ValueError(f"'num_speakers' column contains NaN values for ids: {bad_ids[:10]}")
    combined["num_speakers"] = combined["num_speakers"].astype(int)
    return combined


def summarize_core(df: pd.DataFrame) -> SummaryOutputs:
    """
    Generate core summary statistics for the dataset.

    Returns
    -------
    SummaryOutputs
        Named collection of summary DataFrames.
    """
    rows = []
    for split in sorted(df["split"].unique()):
        subset = df[df["split"] == split]
        total = len(subset)
        num_two = int((subset["num_speakers"] == 2).sum())
        num_one = int((subset["num_speakers"] == 1).sum())
        two_ratio = num_two / total if total else 0.0
        rows.append(
            {
                "split": split,
                "total_clips": total,
                "num_speakers_1": num_one,
                "num_speakers_2": num_two,
                "two_speaker_ratio": round(two_ratio, 4),
            }
        )
    summary_by_split = pd.DataFrame(rows)
    total_global = int(len(df))
    global_row = {
        "split": "global",
        "total_clips": total_global,
        "num_speakers_1": int((df["num_speakers"] == 1).sum()),
        "num_speakers_2": int((df["num_speakers"] == 2).sum()),
    }
    ratio_global = (
        global_row["num_speakers_2"] / global_row["total_clips"] if global_row["total_clips"] else 0.0
    )
    global_row["two_speaker_ratio"] = round(ratio_global, 4)
    summary_global = pd.DataFrame([global_row])

    def distribution_by_split(
        column: str,
        values: Iterable,
        normalize: bool = True,
        subset: Optional[pd.DataFrame] = None,
    ) -> pd.DataFrame:
        data = subset if subset is not None else df
        rows_local: List[Dict[str, object]] = []
        for split in sorted(data["split"].unique()):
            split_df = data[data["split"] == split]
            denom = len(split_df)
            counts = split_df[column].value_counts(dropna=False)
            for value in values:
                count = int(counts.get(value, 0))
                ratio = count / denom if denom and normalize else count
                rows_local.append(
                    {
                        "split": split,
                        column: value,
                        "count": count,
                        "ratio": round(ratio, 4) if normalize else count,
                    }
                )
        return pd.DataFrame(rows_local)

    def distribution_global(column: str, values: Iterable, data: pd.DataFrame) -> pd.DataFrame:
        total = len(data)
        counts = data[column].value_counts(dropna=False)
        rows_global: List[Dict[str, object]] = []
        for value in values:
            count = int(counts.get(value, 0))
            ratio = count / total if total else 0.0
            rows_global.append(
                {
                    "split": "global",
                    column: value,
                    "count": count,
                    "ratio": round(ratio, 4),
                }
            )
        return pd.DataFrame(rows_global)

    coverage_values = sorted(df["coverage_ratio"].dropna().unique().tolist())
    placement_values = sorted(df["placement"].dropna().unique().tolist())
    two_speaker_df = df[df["num_speakers"] == 2]
    overlap_values = sorted(two_speaker_df["overlap_bucket"].dropna().unique().tolist())

    if coverage_values:
        coverage_by_split = pd.concat(
            [
                distribution_by_split("coverage_ratio", coverage_values),
                distribution_global("coverage_ratio", coverage_values, df),
            ],
            ignore_index=True,
        )
    else:
        coverage_by_split = pd.DataFrame(columns=["split", "coverage_ratio", "count", "ratio"])

    if placement_values:
        placement_by_split = pd.concat(
            [
                distribution_by_split("placement", placement_values),
                distribution_global("placement", placement_values, df),
            ],
            ignore_index=True,
        )
    else:
        placement_by_split = pd.DataFrame(columns=["split", "placement", "count", "ratio"])
    if overlap_values:
        overlap_bucket_by_split = pd.concat(
            [
                distribution_by_split(
                    "overlap_bucket", overlap_values, subset=two_speaker_df
                ),
                distribution_global("overlap_bucket", overlap_values, two_speaker_df),
            ],
            ignore_index=True,
        )
    else:
        overlap_bucket_by_split = pd.DataFrame(columns=["split", "overlap_bucket", "count", "ratio"])

    overlap_stats_rows: List[Dict[str, object]] = []
    for split in sorted(df["split"].unique()):
        subset = two_speaker_df[two_speaker_df["split"] == split]
        stats = compute_overlap_stats(subset)
        stats["split"] = split
        overlap_stats_rows.append(stats)
    global_stats = compute_overlap_stats(two_speaker_df)
    global_stats["split"] = "global"
    overlap_stats_rows.append(global_stats)
    overlap_stats = pd.DataFrame(overlap_stats_rows)

    gender_counts = compute_gender_counts(df, two_speaker_df)
    institute_counts = compute_institute_counts(df)

    return SummaryOutputs(
        summary_global=summary_global,
        summary_by_split=summary_by_split,
        coverage_by_split=coverage_by_split,
        placement_by_split=placement_by_split,
        overlap_bucket_by_split=overlap_bucket_by_split,
        overlap_stats=overlap_stats,
        gender_counts=gender_counts,
        institute_counts=institute_counts,
    )


def compute_overlap_stats(df: pd.DataFrame) -> Dict[str, object]:
    """Compute descriptive statistics over overlap_sec for the provided dataframe."""
    if df.empty:
        return {
            "overlap_sec_min": np.nan,
            "overlap_sec_mean": np.nan,
            "overlap_sec_std": np.nan,
            "overlap_sec_p25": np.nan,
            "overlap_sec_median": np.nan,
            "overlap_sec_p75": np.nan,
            "overlap_sec_max": np.nan,
        }
    overlap = df["overlap_sec"].astype(float)
    return {
        "overlap_sec_min": round(float(overlap.min()), 4),
        "overlap_sec_mean": round(float(overlap.mean()), 4),
        "overlap_sec_std": round(float(overlap.std(ddof=0)), 4),
        "overlap_sec_p25": round(float(overlap.quantile(0.25)), 4),
        "overlap_sec_median": round(float(overlap.median()), 4),
        "overlap_sec_p75": round(float(overlap.quantile(0.75)), 4),
        "overlap_sec_max": round(float(overlap.max()), 4),
    }


def compute_gender_counts(df: pd.DataFrame, df_two: pd.DataFrame) -> pd.DataFrame:
    """Aggregate counts for genders and two-speaker gender pairs."""
    genders = pd.concat(
        [
            df["gender_s1"],
            df["gender_s2"],
        ],
        ignore_index=True,
    )
    genders = genders[genders.notna() & (genders != "NONE")]
    gender_counts = genders.value_counts().reset_index()
    gender_counts.columns = ["category", "count"]
    gender_counts["type"] = "single_gender"

    gender_map = {"male": "m", "female": "f"}
    if not df_two.empty:
        pairs = df_two[["gender_s1", "gender_s2"]].dropna()
        pairs = pairs[(pairs["gender_s1"] != "NONE") & (pairs["gender_s2"] != "NONE")]

        def canonical_pair(row) -> str:
            g1 = gender_map.get(str(row["gender_s1"]).lower(), "x")
            g2 = gender_map.get(str(row["gender_s2"]).lower(), "x")
            if g1 == "m" and g2 == "m":
                return "mm"
            if g1 == "f" and g2 == "f":
                return "ff"
            if {"m", "f"} == {g1, g2}:
                return "mf"
            return "other"

        pairs["pair"] = pairs.apply(canonical_pair, axis=1)
        pair_counts = pairs["pair"].value_counts().reset_index()
        pair_counts.columns = ["category", "count"]
        pair_counts["type"] = "two_speaker_pair"
        combined = pd.concat([gender_counts, pair_counts], ignore_index=True)
    else:
        combined = gender_counts
    combined = combined.sort_values(["type", "category"]).reset_index(drop=True)
    return combined


def compute_institute_counts(df: pd.DataFrame) -> pd.DataFrame:
    """Flatten institutes across speakers and compute frequency counts."""
    institutes = pd.concat(
        [
            df["institute_s1"],
            df["institute_s2"],
        ],
        ignore_index=True,
    )
    institutes = institutes[institutes.notna() & (institutes != "NONE")]
    counts = institutes.value_counts().reset_index()
    counts.columns = ["institute", "count"]
    return counts


def run_sanity_checks(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, int]]:
    """
    Perform sanity validations on metadata values.

    Returns
    -------
    pd.DataFrame
        Dataframe listing discovered issues.
    dict
        Aggregated issue counts keyed by issue type.
    """
    issues: List[Dict[str, object]] = []
    def record_issue(issue_type: str, subset: pd.DataFrame, message: str) -> None:
        for _, row in subset.iterrows():
            issues.append(
                {
                    "issue_type": issue_type,
                    "id": row.get("id", ""),
                    "split": row.get("split", ""),
                    "detail": message,
                }
            )

    tol = 1e-6
    single = df[df["num_speakers"] == 1]
    bad_single = single[
        (single["s2_start"].abs() > tol)
        | (single["s2_dur"].abs() > tol)
        | (single["overlap_sec"].abs() > tol)
        | (single["overlap_ratio_window"].abs() > tol)
    ]
    if not bad_single.empty:
        record_issue("single_speaker_noise_fields", bad_single, "Expected zero-valued s2/overlap metrics.")

    s1_bad = df[
        (df["s1_start"] < -tol)
        | (df["s1_dur"] < -tol)
        | ((df["s1_start"] + df["s1_dur"]) > FOUR_SEC_TOL + tol)
    ]
    if not s1_bad.empty:
        record_issue("s1_range_violation", s1_bad, "s1 timing outside [0, 4.001]s bounds.")

    two = df[df["num_speakers"] == 2]
    s2_bad = two[
        (two["s2_start"] < -tol)
        | (two["s2_dur"] <= tol)
        | ((two["s2_start"] + two["s2_dur"]) > FOUR_SEC_TOL + tol)
    ]
    if not s2_bad.empty:
        record_issue("s2_range_violation", s2_bad, "s2 timing outside valid bounds.")

    if not two.empty:
        diff = (two["overlap_ratio_window"] - two["overlap_sec"] / CLIP_DURATION_SECONDS).abs()
        mismatch = two[diff > RATIO_TOL]
        if not mismatch.empty:
            record_issue("overlap_ratio_mismatch", mismatch, "Overlap ratio differs from overlap_sec/4.0 beyond tolerance.")

    sample_rates = df["sample_rate"].dropna().unique()
    if len(sample_rates) > 1:
        issue_df = df.drop_duplicates("sample_rate")
        record_issue("sample_rate_inconsistent", issue_df, f"Multiple sample rates detected: {sample_rates}")
    elif len(sample_rates) == 1 and int(sample_rates[0]) != 16000:
        issue_df = df
        record_issue("sample_rate_unexpected", issue_df.head(1), f"Expected 16000 Hz, observed {sample_rates[0]}")

    issues_df = pd.DataFrame(issues, columns=["issue_type", "id", "split", "detail"])
    counts = issues_df["issue_type"].value_counts().to_dict()
    return issues_df, counts


def safe_parse_expectation(
    spec: Optional[str],
    cast_key,
    default: Dict,
) -> Dict:
    """
    Parse expectation strings while allowing bracketed keys containing commas.

    Parameters
    ----------
    spec : Optional[str]
        Expectation string supplied by the user. Supports comma or semicolon separators.
    cast_key : Callable
        Function used to cast the key portion of each entry.
    default : Dict
        Default expectation mapping to use when spec is None or empty.
    """

    if spec is None:
        return dict(default)

    spec = spec.strip()
    if not spec:
        return dict(default)

    entries: List[str] = []
    buf: List[str] = []
    depth = 0
    bracket_pairs = {"(": ")", "[": "]", "{": "}"}
    closing = {")", "]", "}"}
    for char in spec:
        if char in bracket_pairs:
            depth += 1
        elif char in closing:
            depth = max(0, depth - 1)
        if char in {",", ";"} and depth == 0:
            token = "".join(buf).strip()
            if token:
                entries.append(token)
            buf = []
        else:
            buf.append(char)
    if buf:
        token = "".join(buf).strip()
        if token:
            entries.append(token)

    if not entries:
        return dict(default)

    mapping: Dict = {}
    for entry in entries:
        if ":" not in entry:
            raise ValueError(f"Invalid expectation fragment '{entry}'. Expected 'key:value'.")
        key_str, value_str = entry.split(":", 1)
        key_str = key_str.strip()
        value_str = value_str.strip()
        if not key_str or not value_str:
            raise ValueError(f"Invalid expectation fragment '{entry}'. Empty key or value.")
        try:
            key = cast_key(key_str)
        except Exception as exc:  # pylint: disable=broad-except
            raise ValueError(f"Failed to parse key '{key_str}': {exc}") from exc
        try:
            value = float(value_str)
        except ValueError as exc:
            raise ValueError(f"Failed to parse value '{value_str}' as float: {exc}") from exc
        mapping[key] = value

    total = sum(mapping.values())
    if total <= 0:
        raise ValueError("Expectation probabilities must sum to > 0.")
    normalized = {key: value / total for key, value in mapping.items()}
    return normalized


def compare_to_targets(
    df: pd.DataFrame,
    expect_two_spk: float,
    expect_coverage: Dict[float, float],
    expect_overlap: Dict[str, float],
) -> pd.DataFrame:
    """
    Compare actual dataset ratios against expected values.

    Returns
    -------
    pd.DataFrame
        Rows with metric, scope, category, expected, actual, abs_delta, rel_delta.
    """
    rows: List[Dict[str, object]] = []
    splits = list(sorted(df["split"].unique())) + ["global"]

    for split in splits:
        subset = df if split == "global" else df[df["split"] == split]
        total = len(subset)
        if total == 0:
            continue
        two_count = int((subset["num_speakers"] == 2).sum())
        actual_ratio = two_count / total if total else 0.0
        rows.append(
            build_deviation_row(
                metric="two_speaker_ratio",
                scope=split,
                category="all",
                expected=expect_two_spk,
                actual=actual_ratio,
            )
        )

        coverage_counts = subset["coverage_ratio"].value_counts(normalize=True)
        for cov_val, expected in expect_coverage.items():
            actual = coverage_counts.get(cov_val, 0.0)
            rows.append(
                build_deviation_row(
                    metric="coverage_ratio",
                    scope=split,
                    category=str(cov_val),
                    expected=expected,
                    actual=actual,
                )
            )

        two_subset = subset[subset["num_speakers"] == 2]
        if not two_subset.empty:
            overlap_counts = two_subset["overlap_bucket"].value_counts(normalize=True)
            for bucket, expected in expect_overlap.items():
                actual = overlap_counts.get(bucket, 0.0)
                rows.append(
                    build_deviation_row(
                        metric="overlap_bucket_ratio",
                        scope=split,
                        category=bucket,
                        expected=expected,
                        actual=actual,
                    )
                )
        else:
            for bucket, expected in expect_overlap.items():
                rows.append(
                    build_deviation_row(
                        metric="overlap_bucket_ratio",
                        scope=split,
                        category=bucket,
                        expected=expected,
                        actual=0.0,
                    )
                )

    return pd.DataFrame(rows)


def build_deviation_row(
    metric: str,
    scope: str,
    category: str,
    expected: float,
    actual: float,
) -> Dict[str, object]:
    """Helper to create a deviation record."""
    abs_delta = actual - expected
    rel_delta = abs_delta / expected if expected else np.nan
    return {
        "metric": metric,
        "scope": scope,
        "category": category,
        "expected": round(float(expected), 4),
        "actual": round(float(actual), 4),
        "abs_delta": round(float(abs_delta), 4),
        "rel_delta": round(float(rel_delta), 4) if not np.isnan(rel_delta) else np.nan,
    }


def write_tables(out_dir: Path, summary: SummaryOutputs, issues: pd.DataFrame, deviations: pd.DataFrame) -> None:
    """Persist generated tables as CSV files."""
    out_dir.mkdir(parents=True, exist_ok=True)
    tables = {
        "summary_global.csv": summary.summary_global,
        "summary_by_split.csv": summary.summary_by_split,
        "coverage_by_split.csv": summary.coverage_by_split,
        "placement_by_split.csv": summary.placement_by_split,
        "overlap_bucket_by_split_2spk.csv": summary.overlap_bucket_by_split,
        "overlap_stats_2spk.csv": summary.overlap_stats,
        "gender_counts.csv": summary.gender_counts,
        "institute_counts.csv": summary.institute_counts,
        "sanity_issues.csv": issues,
        "deviation_report.csv": deviations,
    }
    for filename, df in tables.items():
        path = out_dir / filename
        if df is None:
            continue
        if df.empty:
            df.to_csv(path, index=False)
        else:
            df.to_csv(path, index=False)


def write_markdown_report(
    out_dir: Path,
    summary: SummaryOutputs,
    deviations: pd.DataFrame,
    issue_counts: Dict[str, int],
) -> None:
    """Create a Markdown report summarizing key findings."""
    report_path = out_dir / "REPORT.md"
    lines: List[str] = []
    lines.append("# Dataset Analysis Report\n")
    lines.append("## Global Overview\n")
    lines.append(summary.summary_global.to_markdown(index=False))
    lines.append("")

    lines.append("## Split Breakdown\n")
    lines.append(summary.summary_by_split.to_markdown(index=False))
    lines.append("")

    lines.append("## Coverage Distribution by Split\n")
    lines.append(summary.coverage_by_split.to_markdown(index=False))
    lines.append("")

    lines.append("## Placement Distribution by Split\n")
    lines.append(summary.placement_by_split.to_markdown(index=False))
    lines.append("")

    lines.append("## Overlap Buckets (Two-Speaker) by Split\n")
    lines.append(summary.overlap_bucket_by_split.to_markdown(index=False))
    lines.append("")

    lines.append("## Overlap Statistics (Two-Speaker)\n")
    lines.append(summary.overlap_stats.to_markdown(index=False))
    lines.append("")

    lines.append("## Gender Counts\n")
    lines.append(summary.gender_counts.to_markdown(index=False))
    lines.append("")

    top_institutes = summary.institute_counts.head(10)
    lines.append("## Top Institutes by Clip Count\n")
    lines.append(top_institutes.to_markdown(index=False))
    lines.append("")

    lines.append("## Deviations vs Targets\n")
    lines.append(deviations.to_markdown(index=False))
    lines.append("")

    lines.append("## Sanity Check Issues\n")
    if issue_counts:
        lines.append("- Detected issues:")
        for issue_type, count in issue_counts.items():
            lines.append(f"  - **{issue_type}**: {count}")
    else:
        lines.append("No sanity check issues detected.")

    report_path.write_text("\n".join(lines), encoding="utf-8")


def make_plots(
    out_dir: Path,
    summary: SummaryOutputs,
    df: pd.DataFrame,
    two_df: pd.DataFrame,
) -> None:
    """Generate matplotlib plots if requested."""
    import matplotlib.pyplot as plt

    out_dir.mkdir(parents=True, exist_ok=True)

    # Plot num_speakers by split
    if not summary.summary_by_split.empty:
        fig, ax = plt.subplots()
        summary_plot = summary.summary_by_split.set_index("split")[["num_speakers_1", "num_speakers_2"]]
        summary_plot.plot(kind="bar", ax=ax)
        ax.set_ylabel("Count")
        ax.set_title("Number of Speakers by Split")
        fig.tight_layout()
        fig.savefig(out_dir / "plot_num_speakers_by_split.png")
        plt.close(fig)

    if not summary.coverage_by_split.empty:
        fig, ax = plt.subplots()
        pivot_cov = summary.coverage_by_split.pivot(
            index="split", columns="coverage_ratio", values="ratio"
        ).fillna(0)
        pivot_cov.plot(kind="bar", ax=ax)
        ax.set_ylabel("Ratio")
        ax.set_title("Coverage Ratio Distribution by Split")
        fig.tight_layout()
        fig.savefig(out_dir / "plot_coverage_by_split.png")
        plt.close(fig)

    if not summary.overlap_bucket_by_split.empty:
        fig, ax = plt.subplots()
        pivot_overlap = summary.overlap_bucket_by_split.pivot(
            index="split", columns="overlap_bucket", values="ratio"
        ).fillna(0)
        pivot_overlap.plot(kind="bar", ax=ax)
        ax.set_ylabel("Ratio")
        ax.set_title("Overlap Bucket Distribution (2-Speaker) by Split")
        fig.tight_layout()
        fig.savefig(out_dir / "plot_overlap_buckets_2spk_by_split.png")
        plt.close(fig)

    two_pairs = summary.gender_counts[summary.gender_counts["type"] == "two_speaker_pair"]
    if not two_pairs.empty:
        fig, ax = plt.subplots()
        ax.bar(two_pairs["category"], two_pairs["count"])
        ax.set_ylabel("Count")
        ax.set_title("Two-Speaker Gender Pair Counts")
        fig.tight_layout()
        fig.savefig(out_dir / "plot_gender_pairs_2spk.png")
        plt.close(fig)

    if not two_df.empty:
        fig, ax = plt.subplots()
        ax.hist(two_df["overlap_sec"], bins=30, edgecolor="black")
        ax.set_xlabel("Overlap (seconds)")
        ax.set_ylabel("Count")
        ax.set_title("Overlap Duration Distribution (2-Speaker)")
        fig.tight_layout()
        fig.savefig(out_dir / "plot_overlap_sec_hist_2spk.png")
        plt.close(fig)


def console_summary(df: pd.DataFrame, issues: Dict[str, int]) -> None:
    """Print concise summary to stdout."""
    total = len(df)
    two_count = int((df["num_speakers"] == 2).sum())
    ratio = two_count / total if total else 0.0
    print(f"Total clips: {total}")
    print(f"Two-speaker clips: {two_count} ({ratio:.3%})")
    if issues:
        print("Sanity issues detected:")
        for issue, count in issues.items():
            print(f"  - {issue}: {count}")
    else:
        print("No sanity issues detected.")


def main() -> int:
    args = parse_args()
    metadata_dir = Path(args.metadata_dir)
    out_dir = Path(args.out_dir)

    try:
        df = load_csvs(metadata_dir)
    except Exception as exc:  # pylint: disable=broad-except
        print(f"Error loading metadata: {exc}", file=sys.stderr)
        return 1

    two_df = df[df["num_speakers"] == 2]
    summary = summarize_core(df)
    issues_df, issue_counts = run_sanity_checks(df)

    try:
        expect_cov = safe_parse_expectation(
            args.expect_coverage,
            cast_key=float,
            default=DEFAULT_EXPECT_COVERAGE,
        )
        expect_overlap = safe_parse_expectation(
            args.expect_overlap,
            cast_key=str,
            default=DEFAULT_EXPECT_OVERLAP,
        )
    except ValueError as exc:
        print(f"Error parsing expectations: {exc}", file=sys.stderr)
        return 1

    deviations = compare_to_targets(
        df=df,
        expect_two_spk=args.expect_two_spk_ratio,
        expect_coverage=expect_cov,
        expect_overlap=expect_overlap,
    )

    write_tables(out_dir, summary, issues_df, deviations)
    write_markdown_report(out_dir, summary, deviations, issue_counts)

    if args.plots:
        make_plots(out_dir, summary, df, two_df)

    console_summary(df, issue_counts)
    return 0


if __name__ == "__main__":
    sys.exit(main())
