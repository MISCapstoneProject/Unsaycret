import argparse
import json
import os
import pathlib
import threading
import time
import queue
from pathlib import Path
from datetime import datetime as dt, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, Future

# ä¿®å¾© SVML éŒ¯èª¤ï¼šåœ¨å°å…¥ PyTorch ä¹‹å‰è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ["MKL_DISABLE_FAST_MM"] = "1"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import torchaudio
import pyaudio  # type: ignore
import numpy as np
from scipy.signal import resample_poly

from utils.logger import get_logger
from utils.constants import (
    DEFAULT_WHISPER_MODEL,
    DEFAULT_WHISPER_BEAM_SIZE,
    ASR_WIN_SEC,
    ASR_CTX_SEC,
    ASR_SAMPLE_RATE,
    ASR_EDGE_MS,
    ASR_EDGE_CONF,
    ASR_EDGE_MIN_DUR,
    ASR_TAIL_PUNCT_GAP,
    ASR_STREAM_POLICY,
)
from utils.env_config import FORCE_CPU, CUDA_DEVICE_INDEX
from modules.separation.separator import AudioSeparator
from modules.identification.VID_identify_v5 import SpeakerIdentifier
from modules.asr.whisper_asr import WhisperASR
from modules.audio.context_reader import fetch_audio_with_context
from modules.asr.text_utils import (
    compute_cer,
    compute_wer,
    clip_words_to_window,
    edge_sanitize,
    suppress_tail_punct,
    rebuild_text_from_words,
)

logger = get_logger(__name__)

logger.info("ğŸ–¥ GPU available: %s", torch.cuda.is_available())
if torch.cuda.is_available():
    logger.info("   Device: %s", torch.cuda.get_device_name(0))

# ---------- 1. GPU/CPU è¨­å‚™é¸æ“‡ ----------
def init_pipeline_modules():
    """åˆå§‹åŒ– sep / spk / asrï¼Œè€ƒæ…® CUDA è¨­å®šï¼Œä¸¦å›å‚³æ¨¡çµ„å¯¦ä¾‹å€‘"""
    current_cuda_device = CUDA_DEVICE_INDEX  # å»ºç«‹æœ¬åœ°è®Šæ•¸é¿å…ä¿®æ”¹å…¨åŸŸè®Šæ•¸

    if FORCE_CPU:
        use_gpu = False
        logger.info("ğŸ”§ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU")
    else:
        use_gpu = torch.cuda.is_available()
        if use_gpu:
            if current_cuda_device < torch.cuda.device_count():
                torch.cuda.set_device(current_cuda_device)
                logger.info(f"ğŸ¯ è¨­å®š CUDA è¨­å‚™ç´¢å¼•: {current_cuda_device}")
                logger.info(f"   ä½¿ç”¨è¨­å‚™: {torch.cuda.get_device_name(current_cuda_device)}")
            else:
                logger.warning(f"âš ï¸  CUDA è¨­å‚™ç´¢å¼• {current_cuda_device} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­è¨­å‚™ 0")
                current_cuda_device = 0
                torch.cuda.set_device(current_cuda_device)
                logger.info(f"   å·²è¨­å®šç‚ºè¨­å‚™ 0: {torch.cuda.get_device_name(0)}")

    logger.info(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {'cuda:' + str(current_cuda_device) if use_gpu else 'cpu'}")

    sep = AudioSeparator()
    spk = SpeakerIdentifier()
    asr = WhisperASR(model_name=DEFAULT_WHISPER_MODEL, gpu=use_gpu, beam=DEFAULT_WHISPER_BEAM_SIZE)

    return sep, spk, asr,use_gpu

def _timed_call(func, *args):
    t0 = time.perf_counter()
    res = func(*args)
    return res, time.perf_counter() - t0


def process_segment(seg_path: str, t0: float, t1: float, absolute_timestamp: float = None, *, sep=None, spk=None, asr=None) -> dict:
    """Process a single separated segment."""
    logger.info(
        f"ğŸ”§ åŸ·è¡Œç·’ {threading.get_ident()} è™•ç† ({t0:.2f}-{t1:.2f}) â†’ {os.path.basename(seg_path)}"
    )
    
    start = time.perf_counter()
    
    seg_path_obj = Path(seg_path)
    seg_dir = seg_path_obj.parent
    speaker_file = seg_path_obj.name
    seg_root_dir = seg_dir.parent if seg_dir is not None else None

    #æ˜¯å¦è·¨çª—0.6ç§’ä¸Šä¸‹æ–‡
    USE_CONTEXT_ASR = True  # True = current behavior; False = legacy no-context

    seg_idx = None
    if seg_dir is not None:
        seg_name = seg_dir.name
        try:
            seg_idx = int(seg_name.split("_")[-1])
        except Exception:
            seg_idx = None
    if seg_idx is None and ASR_WIN_SEC:
        try:
            seg_idx = int(round(t0 / ASR_WIN_SEC))
        except Exception:
            seg_idx = None

    def _asr_with_context():
        if seg_root_dir is None or seg_idx is None:
            text, conf, raw_words = asr.transcribe(seg_path, language="zh")
            abs_words = []
            for w in raw_words:
                w2 = dict(w)
                w2["start"] = float(w2.get("start", 0.0)) + t0
                w2["end"] = float(w2.get("end", 0.0)) + t0
                abs_words.append(w2)
            words = clip_words_to_window(abs_words, t0, t1)
            words = edge_sanitize(words, t0, t1, ASR_EDGE_MS, ASR_EDGE_CONF, ASR_EDGE_MIN_DUR)
            final_text = rebuild_text_from_words(words) or text
            if words:
                last_end = float(words[-1].get("end", t0))
                final_text = suppress_tail_punct(final_text, last_end, t1, ASR_TAIL_PUNCT_GAP)
            return final_text, conf, words

        next_path = seg_root_dir / f"segment_{seg_idx + 1:03d}" / speaker_file
        if ASR_STREAM_POLICY == "SMALL_LOOKAHEAD":
            deadline = time.perf_counter() + ASR_CTX_SEC
            while not next_path.exists() and time.perf_counter() < deadline:
                time.sleep(0.05)

        buf = fetch_audio_with_context(
            base_dir=str(seg_root_dir),
            seg_idx=seg_idx,
            speaker_file=speaker_file,
            win_sec=ASR_WIN_SEC,
            ctx_sec=ASR_CTX_SEC,
            sr=ASR_SAMPLE_RATE,
        )
        _full_text, avg_conf, word_info = asr.transcribe_tensor(buf, language="zh")

        offset = t0 - ASR_CTX_SEC
        contextual_words = []
        for w in word_info:
            w2 = dict(w)
            w2["start"] = float(w2.get("start", 0.0)) + offset
            w2["end"] = float(w2.get("end", 0.0)) + offset
            contextual_words.append(w2)

        words = clip_words_to_window(contextual_words, t0, t1)
        words = edge_sanitize(words, t0, t1, ASR_EDGE_MS, ASR_EDGE_CONF, ASR_EDGE_MIN_DUR)
        final_text = rebuild_text_from_words(words)
        if words:
            last_end = float(words[-1].get("end", t0))
            final_text = suppress_tail_punct(final_text, last_end, t1, ASR_TAIL_PUNCT_GAP)
        else:
            final_text = final_text or ""

        return final_text, avg_conf, words

    def _asr_no_context():
        # No look-ahead, no contextual buffer; preserve downstream invariants
        text, conf, raw_words = asr.transcribe(seg_path, language="zh")

        # Convert to absolute word times
        abs_words = []
        for w in (raw_words or []):
            w2 = dict(w)
            w2["start"] = float(w2.get("start", 0.0)) + t0
            w2["end"] = float(w2.get("end", 0.0)) + t0
            abs_words.append(w2)

        # Clip to current window and sanitize edges
        words = clip_words_to_window(abs_words, t0, t1)
        words = edge_sanitize(words, t0, t1, ASR_EDGE_MS, ASR_EDGE_CONF, ASR_EDGE_MIN_DUR)

        # Rebuild text and suppress unstable tail punctuation
        final_text = rebuild_text_from_words(words) or (text or "")
        if words:
            last_end = float(words[-1].get("end", t0))
            final_text = suppress_tail_punct(final_text, last_end, t1, ASR_TAIL_PUNCT_GAP)

        return final_text, conf, words

    with ThreadPoolExecutor(max_workers=2) as ex:
        spk_future = ex.submit(_timed_call, spk.process_audio_file, seg_path)
        if USE_CONTEXT_ASR:
            asr_future = ex.submit(_timed_call, _asr_with_context)
        else:
            asr_future = ex.submit(_timed_call, _asr_no_context)

        # å®‰å…¨åœ°å–å¾—èªè€…è­˜åˆ¥çµæœ
        spk_result = spk_future.result()
        if spk_result is None:
            logger.error(f"èªè€…è­˜åˆ¥å¤±æ•—: {seg_path}")
            speaker_id, name, dist = "unknown", "Unknown", 999.0
            spk_time = 0.0
        else:
            (speaker_id, name, dist), spk_time = spk_result
            
        (text, conf, words), asr_time = asr_future.result()

    #æ¨™é»ç¬¦è™Ÿèˆ‡ç°¡å–®ä¿®å­—
    USE_PUNCTUATOR = False  # True = enable ChinesePunctuator cleanup; False = skip entirely

    if USE_PUNCTUATOR:
        # Immediate cleanup for Chinese ASR text (fail-open on errors)
        try:
            if not hasattr(process_segment, "_punctuator_lock"):
                process_segment._punctuator_lock = threading.Lock()

            if not hasattr(process_segment, "_punctuator"):
                with process_segment._punctuator_lock:
                    if not hasattr(process_segment, "_punctuator"):
                        from modules.text.punctuator import ChinesePunctuator
                        device = "cuda" if getattr(asr, "gpu", False) else "cpu"
                        process_segment._punctuator = ChinesePunctuator(device=device)

            punctuator = getattr(process_segment, "_punctuator", None)
            if punctuator is not None:
                text = punctuator.apply(text, use_macbert=True)
        except Exception:
            logger.debug("Chinese punctuator cleanup skipped due to error.", exc_info=True)

    logger.info(f"â± SpeakerID è€—æ™‚ {spk_time:.3f}s")
    logger.info(f"â± ASR è€—æ™‚ {asr_time:.3f}s")

    # Word timestamps already absolute; copy for downstream consumers.
    adjusted_words = [dict(w) for w in (words or [])]

    total = time.perf_counter() - start
    logger.info(f"â± segment ç¸½è€—æ™‚ {total:.3f}s")

    result = {
        "start": round(t0, 2),
        "end": round(t1, 2),
        "speaker": name,
        "speaker_id": speaker_id,
        "distance": round(float(dist), 3),
        "text": text,
        "confidence": round(conf, 2),
        "words": adjusted_words,
        "spk_time": spk_time,
        "asr_time": asr_time,
        "path": seg_path
    }
    
    # å¦‚æœæœ‰çµ•å°æ™‚é–“æˆ³ï¼ŒåŠ å…¥åˆ°çµæœä¸­
    if absolute_timestamp is not None:
        result["absolute_timestamp"] = absolute_timestamp
        # ä½¿ç”¨å°åŒ—æ™‚é–“æˆ³è½‰æ› (UTC+8)
        taipei_tz = timezone(timedelta(hours=8))
        result["absolute_start_time"] = dt.fromtimestamp(absolute_timestamp, tz=taipei_tz).isoformat()
        result["absolute_end_time"] = dt.fromtimestamp(absolute_timestamp + (t1 - t0), tz=taipei_tz).isoformat()
    
    return result



def make_pretty(seg: dict) -> dict:
    """Convert a segment dict to human friendly format."""
    return {
        "time": f"{seg['start']:.2f}s â†’ {seg['end']:.2f}s",
        "speaker": seg["speaker"],
        "similarity": f"{seg['distance']:.3f}",
        "confidence": f"{seg['confidence']*100:.1f}%",
        "text": seg["text"],
        "word_count": len(seg["words"]),
    }


def run_pipeline_file(raw_wav: str, max_workers: int = 3, sep=None, spk=None, asr=None):
    """Run pipeline on an existing wav file.

    If the separator / speaker identifier / ASR modules are not provided,
    they will be initialized automatically. This preserves backwards
    compatibility for callers that import :func:`run_pipeline_file` directly
    without using :func:`init_pipeline_modules` first (e.g. older API code).
    """

    # Allow legacy usage where modules are not injected explicitly.
    if sep is None or spk is None or asr is None:
        sep, spk, asr, _ = init_pipeline_modules()

    total_start = time.perf_counter()

    waveform, sr = torchaudio.load(raw_wav)
    
    # ç¢ºä¿å–®è²é“ï¼šå¦‚æœæ˜¯å¤šè²é“ï¼Œè½‰ç‚ºå–®è²é“ï¼ˆå–å¹³å‡ï¼‰
    if waveform.shape[0] > 1:
        logger.info(f"ğŸ”„ å¤šè²é“éŸ³æª” ({waveform.shape[0]} è²é“) â†’ å–®è²é“")
        waveform = torch.mean(waveform, dim=0, keepdim=True)
    
    # å¦‚æœæ¡æ¨£ç‡ä¸ç­‰æ–¼16000å°±é‡æ¡æ¨£ï¼ˆä½¿ç”¨é«˜å“è³ª scipy resample_polyï¼‰
    if sr != 16000:
        logger.info(f"ğŸ”„ æ¡æ¨£ç‡ {sr} â‰  16000ï¼Œé€²è¡Œé‡æ¡æ¨£")
        # è½‰æ›ç‚º numpy é€²è¡Œé«˜å“è³ªé‡æ¡æ¨£
        waveform_np = waveform.cpu().numpy()
        # æ­¤æ™‚å·²ç¢ºä¿æ˜¯å–®è²é“ï¼Œç›´æ¥è™•ç†
        resampled = resample_poly(waveform_np.squeeze(), 16000, sr)
        waveform = torch.from_numpy(resampled).unsqueeze(0)
        sr = 16000
    
    # â† æŠŠ waveform å‚³åˆ° separator è¨­å®šçš„è£ç½® (cuda or cpu)
    waveform = waveform.to(sep.device)
    audio_len = waveform.shape[1] / sr
    out_dir = pathlib.Path("work_output") / dt.now().strftime("%Y%m%d_%H%M%S")
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) åˆ†é›¢
    sep_start = time.perf_counter()
    file_start_time = dt.now()  # è¨˜éŒ„è™•ç†æª”æ¡ˆçš„é–‹å§‹æ™‚é–“
    segments = sep.separate_and_save(waveform, str(out_dir), segment_index=0, absolute_start_time=file_start_time)
    if not segments:                           # â† æ–°å¢
        logger.error("ğŸš¨ èªè€…åˆ†é›¢å¤±æ•—ï¼šå›å‚³ç©ºå€¼ / None")
        raise RuntimeError("Speaker separation failed â€“ no segments returned")
    sep_end = time.perf_counter()
    logger.info(f"â± åˆ†é›¢è€—æ™‚ {sep_end - sep_start:.3f}s, å…± {len(segments)} æ®µ")

    # 2) å¤šåŸ·è¡Œç·’è™•ç†æ‰€æœ‰æ®µï¼ˆåŒæ™‚æ”¯æ´å«/ä¸å« absolute_timestampï¼Œä¸¦æ³¨å…¥ sep/spk/asrï¼‰
    # é‡‹æ”¾åˆ†é›¢æ­¥é©Ÿæš«ä½”çš„ VRAMï¼Œé¿å…å¾ŒçºŒ ASR/SpkID çˆ†é¡¯å­˜
    if torch.cuda.is_available():
        try:
            torch.cuda.empty_cache()
        except Exception as e:
            logger.debug(f"torch.cuda.empty_cache skipped: {e}")

    logger.info(f"ğŸ”„ è™•ç† {len(segments)} æ®µ... (max_workers={max_workers})")


    def _run(seg):
        if len(seg) >= 4:
            # æ–°æ ¼å¼ï¼š(path, start, end, absolute_timestamp)
            return process_segment(seg[0], seg[1], seg[2], seg[3],
                                sep=sep, spk=spk, asr=asr)
        else:
            # èˆŠæ ¼å¼ï¼š(path, start, end)
            return process_segment(seg[0], seg[1], seg[2],
                                sep=sep, spk=spk, asr=asr)

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        bundle = [
            r for r in ex.map(_run, segments) if r
        ]

    spk_time = max((s.get("spk_time", 0.0) for s in bundle), default=0.0)
    asr_time = max((s.get("asr_time", 0.0) for s in bundle), default=0.0)
    pipeline_total = (sep_end - sep_start) + spk_time + asr_time



    stages_path = pathlib.Path("pipeline_stages.csv")
    if not stages_path.exists():
        stages_path.write_text("sep_max,sid_max,asr_max,pipeline_total\n", encoding="utf-8")
    with stages_path.open("a", encoding="utf-8") as f:
        f.write(f"{sep_end - sep_start:.3f},{spk_time:.3f},{asr_time:.3f},{pipeline_total:.3f}\n")

    # 3) è¼¸å‡ºçµæœ + ASR å“è³ªæŒ‡æ¨™
    bundle.sort(key=lambda x: x["start"])
    pretty_bundle = [make_pretty(s) for s in bundle]

    # ### PATCH START: quality metrics ###
    valid = [s for s in bundle if s.get("text")]
    avg_conf = (sum(s.get("confidence", 0.0) for s in valid) / len(valid)) if valid else 0.0
    recog_text = " ".join(s.get("text", "") for s in valid)

    ref_text = None
    wav_path = pathlib.Path(raw_wav)
    for ext in (".txt", ".lab"):
        p = wav_path.with_suffix(ext)
        if p.exists():
            ref_text = p.read_text(encoding="utf-8").strip()
            break

    if ref_text is not None:
        wer = compute_wer(ref_text, recog_text)
        cer = compute_cer(ref_text, recog_text)
    else:
        wer = None
        cer = None
    # ### PATCH END ###

    json_path = out_dir / "output.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump({"segments": bundle}, f, ensure_ascii=False, indent=2)

    total_end = time.perf_counter()
    total_time = total_end - total_start
    logger.info(f"âœ… Pipeline finished â†’ {json_path} (ç¸½è€—æ™‚ {total_time:.3f}s)")

    stats = {
        "length": audio_len,
        "total": total_time,
        "separate": sep_end - sep_start,
        "speaker": spk_time,
        "asr": asr_time,
        "pipeline_total": pipeline_total,
        "avg_conf": avg_conf,
        "wer": wer,
        "cer": cer,
    }

    return bundle, pretty_bundle, stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dir Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_truth_map(path: str) -> dict[str, str]:
    """è®€å– filename<TAB>transcript çš„å°ç…§è¡¨ã€‚"""
    m: dict[str, str] = {}
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or "\t" not in line:
                continue
            fname, txt = line.split("\t", 1)
            m[fname] = txt.strip()
    return m


def run_pipeline_dir(
    dir_path: str,
    truth_map_path: str = "truth_map.txt",
    max_workers: int = 3, sep=None, spk=None, asr=None
) -> str:
    """
    æ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾å…§æ‰€æœ‰éŸ³æª”ï¼Œè¼¸å‡ºï¼š
      - summary.tsvï¼šæª”æ¡ˆç´šçµ±è¨ˆ + æ®µè½è©³æƒ…
      - asr_report.tsvï¼šASR æŒ‡æ¨™ (avg_conf, WER, CER)
    """
    
    # Lazily initialize modules for legacy callers that do not provide them.
    if sep is None or spk is None or asr is None:
        sep, spk, asr, _ = init_pipeline_modules()

    timestamp = dt.now().strftime("%Y%m%d_%H%M%S")
    out_dir = pathlib.Path("work_output") / f"batch_{timestamp}"
    out_dir.mkdir(parents=True, exist_ok=True)
    summary_path = out_dir / "summary.tsv"
    asr_report_path = out_dir / "asr_report.tsv"

    truth_map = load_truth_map(truth_map_path) if truth_map_path and os.path.exists(truth_map_path) else {}

    audio_files = [
        f for f in pathlib.Path(dir_path).rglob("*")
        if f.suffix.lower() in {".wav", ".mp3", ".flac", ".ogg"}
    ]
    if not audio_files:
        logger.warning("âš ï¸  ç›®éŒ„å…§æœªæ‰¾åˆ°æ”¯æ´æ ¼å¼éŸ³æª”")
        return str(summary_path)

    file_results: list[tuple[int, Path, dict, list[dict]]] = []
    for idx, audio in enumerate(sorted(audio_files), start=1):
        logger.info(f"===== è™•ç†æª”æ¡ˆ {audio.name} ({idx}/{len(audio_files)}) =====")
        segments, pretty, stats = run_pipeline_file(str(audio), max_workers , sep=sep, spk=spk, asr=asr)

        # ç”¨ truth_map è¦†å¯« WER/CER
        gt = truth_map.get(audio.name)
        if gt:
            recog_text = " ".join(s.get("text", "") for s in segments if s.get("text"))
            stats["wer"] = compute_wer(gt, recog_text)
            stats["cer"] = compute_cer(gt, recog_text)

        file_results.append((idx, audio, stats, segments))

    with open(summary_path, "w", encoding="utf-8") as f_sum, \
         open(asr_report_path, "w", encoding="utf-8") as f_asr:

        # æª”æ¡ˆç´šçµ±è¨ˆ
        f_sum.write("ç·¨è™Ÿ\tæª”å\téŸ³æª”é•·åº¦(s)\tç¸½è€—æ™‚(s)\tåˆ†é›¢è€—æ™‚(s)\tSpeakerIDè€—æ™‚(s)\tASRè€—æ™‚(s)\n")
        # ASR å ±è¡¨
        f_asr.write("ç·¨è™Ÿ\tæª”å\tASRè€—æ™‚(s)\tç¸½è€—æ™‚(s)\tå¹³å‡confidence\tWER\tCER\n")

        for idx, audio, stats, segments in file_results:
            wer_str = f"{stats['wer']:.4f}" if stats.get("wer") is not None else "NA"
            cer_str = f"{stats['cer']:.4f}" if stats.get("cer") is not None else "NA"

            f_sum.write(
                f"{idx}\t{audio.name}\t{stats['length']:.2f}\t{stats['total']:.2f}\t"
                f"{stats['separate']:.2f}\t{stats['speaker']:.2f}\t{stats['asr']:.2f}\n"
            )
            f_asr.write(
                f"{idx}\t{audio.name}\t{stats['asr']:.2f}\t{stats['total']:.2f}\t"
                f"{stats.get('avg_conf', 0.0):.4f}\t{wer_str}\t{cer_str}\n"
            )

        # æ®µè½è©³æƒ…
        f_sum.write("\næª”æ¡ˆ\té–‹å§‹(s)\tçµæŸ(s)\tèªªè©±è€…\tdistance\tconfidence\tæ–‡å­—\n")
        for _, audio, _, segments in file_results:
            for seg in segments:
                text = seg.get("text", "").replace("\t", " ")
                f_sum.write(
                    f"{audio.name}\t{seg['start']:.3f}\t{seg['end']:.3f}\t"
                    f"{seg['speaker']}\t{seg.get('distance', 0.0):.4f}\t"
                    f"{seg['confidence']:.4f}\t{text}\n"
                )

    logger.info(f"âœ… Directory pipeline å®Œæˆ â†’ {summary_path}")
    logger.info(f"ğŸ“Š ASR report â†’ {asr_report_path}")
    return str(summary_path)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Stream Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_pipeline_stream(
    chunk_secs: float = 4.0,
    rate: int = 16000,
    channels: int = 1,
    frames_per_buffer: int = 1024,
    max_workers: int = 2,
    record_secs: float | None = None,
    queue_out: "queue.Queue[dict] | None" = None,
    stop_event: threading.Event | None = None,
    in_bytes_queue: "queue.Queue[bytes] | None" = None,
    sep=None, spk=None, asr=None
):
    """ä¸²æµæ¨¡å¼ï¼šæ¯ chunk_secs åšä¸€æ¬¡åˆ†é›¢/è­˜åˆ¥/ASRã€‚"""

    # Initialize modules when not supplied to maintain backward compatibility
    if sep is None or spk is None or asr is None:
        sep, spk, asr, _ = init_pipeline_modules()

    total_start = time.perf_counter()
    out_root = Path("stream_output") / dt.now().strftime("%Y%m%d_%H%M%S")
    out_root.mkdir(parents=True, exist_ok=True)

    executor: ThreadPoolExecutor = ThreadPoolExecutor(max_workers=max_workers)
    futures: list[Future] = []
    
    # ä½¿ç”¨å°åŒ—æ™‚é–“ä½œç‚ºä¸²æµé–‹å§‹æ™‚é–“ (UTC+8)
    taipei_tz = timezone(timedelta(hours=8))
    stream_start_time = dt.now(taipei_tz)

    def process_chunk(raw_bytes: bytes, idx: int, src_sr: int, chunk_start_time: dt = None):
        try:
            t0 = idx * chunk_secs
            t1 = t0 + chunk_secs

            BYTES_PER_SAMPLE = 4
            if channels <= 0 or chunk_secs <= 0:
                raise ValueError(f"invalid channels={channels} or chunk_secs={chunk_secs}")

            # B) decode & è§£äº¤éŒ¯ â†’ [C, T]
            n_samples_total = len(raw_bytes) // (BYTES_PER_SAMPLE * channels)
            if n_samples_total == 0:
                logger.warning(f"[proc] idx={idx} empty chunk: bytes={len(raw_bytes)}")
                return None

            waveform = torch.frombuffer(raw_bytes, dtype=torch.float32)
            waveform = waveform[: n_samples_total * channels].view(channels, n_samples_total)
            if channels > 1:
                waveform = waveform.mean(dim=0, keepdim=True)   # [1, T]
            # channels==1 æ™‚å·²æ˜¯ [1, T]ï¼Œä¸è¦å† unsqueeze

            # C) é‡æ¡æ¨£åˆ° 16k çµ¦æ¨¡å‹
            MODEL_SR = 16000
            wave_for_model = waveform
            if src_sr != MODEL_SR:
                wave_for_model = torchaudio.transforms.Resample(orig_freq=src_sr, new_freq=MODEL_SR)(waveform)

            # D) å­˜å…©ä»½æ··éŸ³ï¼šä¸€ä»½ 48k/44.1k çµ¦è€³æœµè½ï¼ˆä¸æœƒè®Šæ…¢ï¼‰ï¼Œä¸€ä»½ 16k çµ¦æ¨¡å‹æª¢æŸ¥
            seg_dir = out_root / f"segment_{idx:03d}"
            seg_dir.mkdir(parents=True, exist_ok=True)
            torchaudio.save((seg_dir / "mix_playback.wav").as_posix(),
                            torch.clamp(waveform * 0.98, -1, 1), src_sr)
            torchaudio.save((seg_dir / "mix_16k.wav").as_posix(),
                            torch.clamp(wave_for_model * 0.98, -1, 1), MODEL_SR)
            mix_path = seg_dir / "mix.wav"
            if chunk_start_time is None:
                chunk_start_time = stream_start_time + timedelta(seconds=t0)

            # F) åˆ†é›¢ï¼ˆåƒ 16kï¼‰
            segments = sep.separate_and_save(
                wave_for_model, seg_dir.as_posix(), segment_index=idx, absolute_start_time=chunk_start_time
            )

            speaker_paths = sorted(seg_dir.glob("speaker*.wav"))
            if not speaker_paths:
                logger.warning("segment %d ç„¡ speaker wav", idx)
                return None

            # --- G) è·‘ SpkID + ASR
            speaker_results: list[dict] = []
            for sp_idx, wav_path in enumerate(speaker_paths, 1):
                if segments and len(segments) > sp_idx - 1 and len(segments[sp_idx - 1]) == 4:
                    absolute_timestamp = segments[sp_idx - 1][3]
                    res = process_segment(
                        str(wav_path), t0, t1, absolute_timestamp,
                        sep=sep, spk=spk, asr=asr
                    )
                else:
                    res = process_segment(
                        str(wav_path), t0, t1,
                        sep=sep, spk=spk, asr=asr
                    )

                if not res["text"].strip() or res["confidence"] < 0.1:
                    continue
                res["speaker_index"] = sp_idx
                speaker_results.append(res)

            # å»é‡ï¼šåŒ speaker ç•™æœ€é«˜ä¿¡å¿ƒ
            unique: dict[str, dict] = {}
            for item in speaker_results:
                n = item["speaker"]
                if n not in unique or item["confidence"] > unique[n]["confidence"]:
                    unique[n] = item
            speaker_results = list(unique.values())

            seg_dict = {
                "segment": idx,
                "start": round(t0, 2),
                "end": round(t1, 2),
                "mix": mix_path.as_posix(),
                "sources": [str(p) for p in speaker_paths],
                "speakers": speaker_results,
            }

            with open(seg_dir / "output.json", "w", encoding="utf-8") as f:
                json.dump(seg_dict, f, ensure_ascii=False, indent=2)

            if queue_out is not None:
                queue_out.put(seg_dict)
            return seg_dict

        except Exception as e:
            logger.exception(f"[process_chunk] idx={idx} failed: {e}")
            return None


    # éŒ„éŸ³/æ¥æ”¶åŸ·è¡Œç·’
    q: queue.Queue[tuple[bytes, int]] = queue.Queue(maxsize=max_workers * 2)
    
    # çµ±ä¸€ä½¿ç”¨å¤–éƒ¨ stop_eventï¼Œå¦‚æœæ²’æä¾›å‰‡å‰µå»ºä¸€å€‹
    if stop_event is None:
        stop_event = threading.Event()

    def recorder_from_queue():
        BYTES_PER_SAMPLE = 4  # Float32
        ch = max(1, int(channels))

        buf = bytearray()
        idx = 0
        start_time = time.time()

        # å…ˆç”¨åƒæ•¸ç•¶æš«å€¼ï¼›é€²ç·š 0.3s å¾Œç”¨ã€Œç‰†é˜æ™‚é–“ + ç´¯ç© bytesã€æ ¡æº–
        provisional_sr = max(1, int(rate))
        frames_needed = int(provisional_sr * chunk_secs) * BYTES_PER_SAMPLE * ch

        calibrated = False
        calib_t0 = time.time()
        calib_bytes = 0

        while not stop_event.is_set():
            try:
                pkt = in_bytes_queue.get(timeout=0.05)  # ç¸®çŸ­ç­‰å¾…æ™‚é–“æå‡éŸ¿æ‡‰æ€§
            except queue.Empty:
                if record_secs is not None and time.time() - start_time >= record_secs:
                    stop_event.set()  # çµ±ä¸€ä½¿ç”¨ stop_event
                    break
                # æ¯æ¬¡ timeout éƒ½æª¢æŸ¥åœæ­¢ä¿¡è™Ÿï¼Œæå‡éŸ¿æ‡‰æ€§
                if stop_event.is_set():
                    logger.info("ğŸ›‘ recorder_from_queue: åµæ¸¬åˆ°åœæ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨é€€å‡º...")
                    break
                continue
            
            # æ”¶åˆ°è³‡æ–™å¾Œä¹Ÿè¦æª¢æŸ¥åœæ­¢ä¿¡è™Ÿå’Œæ˜¯å¦ç‚ºçµæŸæ¨™è¨˜
            if stop_event.is_set():
                logger.info("ğŸ›‘ recorder_from_queue: æ”¶åˆ°è³‡æ–™å¾Œåµæ¸¬åˆ°åœæ­¢ä¿¡è™Ÿ")
                break
            
            # è™•ç†çµæŸæ¨™è¨˜ï¼ˆç©º bytes æˆ– Noneï¼‰
            if pkt is None or len(pkt) == 0:
                logger.info("ğŸ recorder_from_queue: æ”¶åˆ°çµæŸæ¨™è¨˜")
                break

            calib_bytes += len(pkt)    # ç”¨ä¾†ä¼°è¨ˆä¾†æº sr
            buf.extend(pkt)

            # ===== é¦–åŒ…æ ¡æº–ï¼šç”¨ bytes/sec ä¼°è¨ˆçœŸå¯¦ä¾†æº srï¼Œä¸€æ¬¡åˆ°ä½ =====
            if not calibrated:
                elapsed = max(1e-3, time.time() - calib_t0)
                if elapsed >= 0.30 and calib_bytes >= BYTES_PER_SAMPLE * ch * 4000:  # è‡³å°‘ ~0.25s@16k çš„é‡
                    bps = calib_bytes / elapsed
                    est_sr = bps / (BYTES_PER_SAMPLE * ch)   # â‰ˆ çœŸå¯¦ sr
                    # å°±è¿‘å°é½Šåˆ°å¸¸è¦‹å–æ¨£ç‡
                    candidates = [48000, 44100, 32000, 24000, 22050, 16000]
                    src_sr = min(candidates, key=lambda s: abs(s - est_sr))
                    frames_needed = int(src_sr * chunk_secs) * BYTES_PER_SAMPLE * ch
                    calibrated = True
                    logger.info(f"[calib] estâ‰ˆ{est_sr:.1f} â†’ use {src_sr}, ch={ch}, frames_needed={frames_needed}")

            # ===== åˆ‡å¡Šï¼šæ ¡æº–å®Œæˆå¾Œæ‰åˆ‡ï¼Œç¢ºä¿æ¯å¡Šæ°å¥½æ˜¯ chunk_secs çš„ã€ŒçœŸå¯¦æ™‚é–“ã€ =====
            if not calibrated:
                continue  # ç­‰æ ¡æº–å®Œæˆå†åˆ‡ï¼Œé¿å…ä¸€é–‹å§‹å°±ç”¨éŒ¯çš„ 16k å°ºå¯¸

            while len(buf) >= frames_needed:
                raw = bytes(buf[:frames_needed])
                del buf[:frames_needed]
                # æŠŠã€Œæ ¡æº–å¾—åˆ°çš„ä¾†æº srã€ä¸€ä½µå‚³ä¸‹å»
                q.put((raw, idx, src_sr))
                idx += 1


    def recorder_from_mic():
        pa = pyaudio.PyAudio()
        stream = pa.open(
            format=pyaudio.paFloat32,        # â† æ”¹æˆ Float32
            channels=channels,
            rate=rate,
            input=True,
            frames_per_buffer=frames_per_buffer,
        )
        BYTES_PER_SAMPLE = 4               # â† èˆ‡ process_chunk ä¸€è‡´
        frames_needed = int(rate * chunk_secs)
        buf = bytearray()
        idx = 0
        start_time = time.time()
        try:
            while not stop_event.is_set():
                if record_secs is not None and time.time() - start_time >= record_secs:
                    stop_event.set()
                    break
                buf.extend(stream.read(frames_per_buffer, exception_on_overflow=False))
                # Float32: æ¯æ¨£æœ¬ 4 bytes
                if len(buf) // BYTES_PER_SAMPLE >= frames_needed:
                    need_bytes = frames_needed * BYTES_PER_SAMPLE
                    raw = bytes(buf[:need_bytes])
                    buf = buf[need_bytes:]
                    q.put((raw, idx, rate))     # â† ä¸€æ¬¡æ”¾ä¸‰å€‹å€¼ (raw, idx, src_sr)
                    idx += 1
        finally:
            stream.stop_stream()
            stream.close()
            pa.terminate()


    rec_thread = threading.Thread(
        target=recorder_from_queue if in_bytes_queue else recorder_from_mic,
        daemon=True,
    )
    rec_thread.start()

    logger.info(
        "ğŸ™ é–‹å§‹éŒ„éŸ³/æ¥æ”¶ (%s) ...",
        "å¤–éƒ¨ bytes" if in_bytes_queue else ("Ctrlâ€‘C" if record_secs is None else f"{record_secs}s"),
    )

    try:
        while True:
            if stop_event and stop_event.is_set():
                logger.info("ğŸ›‘ ä¸»å¾ªç’°: åµæ¸¬åˆ°åœæ­¢ä¿¡è™Ÿï¼Œæº–å‚™çµæŸ")
                break
            try:
                raw, idx, src_sr = q.get(timeout=0.05)  # ç¸®çŸ­ timeout æå‡éŸ¿æ‡‰æ€§
            except queue.Empty:
                if stop_event.is_set():
                    logger.info("ğŸ›‘ ä¸»å¾ªç’°: ä½‡åˆ—ç‚ºç©ºä¸”æ”¶åˆ°åœæ­¢ä¿¡è™Ÿ")
                    break
                continue
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºçµæŸæ¨™è¨˜
            if not raw or len(raw) == 0:
                logger.info("ğŸ ä¸»å¾ªç’°: æ”¶åˆ°çµæŸæ¨™è¨˜")
                break
                
            futures.append(executor.submit(process_chunk, raw, idx, src_sr))
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Ctrlâ€‘C åµæ¸¬åˆ°ä½¿ç”¨è€…æ‰‹å‹•åœæ­¢")
        stop_event.set()  # çµ±ä¸€ä½¿ç”¨ stop_event
    finally:
        logger.info("ğŸ§¹ é–‹å§‹æ¸…ç† pipeline è³‡æº")
        stop_event.set()  # ç¢ºä¿åœæ­¢ä¿¡è™Ÿè¢«è¨­ç½®
        
        # ç™¼é€çµæŸæ¨™è¨˜å–šé†’å¯èƒ½é˜»å¡çš„ recorder ç·šç¨‹
        try:
            if in_bytes_queue:
                in_bytes_queue.put_nowait(b"")
            q.put_nowait((b"", -1, 16000))  # çµæŸæ¨™è¨˜
        except:
            pass
        
        # ç­‰å¾… recorder ç·šç¨‹çµæŸ
        logger.info("â³ ç­‰å¾… recorder ç·šç¨‹çµæŸ...")
        rec_thread.join(timeout=2)
        if rec_thread.is_alive():
            logger.warning("âš ï¸  recorder ç·šç¨‹æœªåœ¨æ™‚é™å…§çµæŸ")
        else:
            logger.info("âœ… recorder ç·šç¨‹å·²çµæŸ")
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œå®Œæˆ
        logger.info("â³ ç­‰å¾…æ‰€æœ‰è™•ç†å·¥ä½œå®Œæˆ...")
        executor.shutdown(wait=True)
        logger.info("âœ… Pipeline æ¸…ç†å®Œæˆ")

    # ç­‰å¾…æ‰€æœ‰ Future å®Œæˆï¼Œç¢ºä¿ä¸ä¸Ÿå¤±ä»»ä½•è™•ç†çµæœ
    bundle = []
    for f in futures:
        try:
            result = f.result()  # ç­‰å¾… Future å®Œæˆ
            if result:
                bundle.append(result)
        except Exception as e:
            logger.error(f"âŒ Future è™•ç†å¤±æ•—: {e}")
    
    bundle.sort(key=lambda x: x["start"])

    pretty_bundle: list[dict] = []
    for seg in bundle:
        for sp in seg["speakers"]:
            pretty_bundle.append(make_pretty(sp))

    logger.info(
        "ğŸš© stream çµæŸï¼Œå…± %d æ®µï¼Œè€—æ™‚ %.3fs â†’ %s",
        len(bundle), time.perf_counter() - total_start, out_root
    )
    return bundle, pretty_bundle


# å…¼å®¹èˆŠåç¨±
run_pipeline_FILE = run_pipeline_file
run_pipeline_STREAM = run_pipeline_stream
run_pipeline_DIR = run_pipeline_dir

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    sep, spk, asr, use_gpu = init_pipeline_modules()
    parser = argparse.ArgumentParser(description="Speech pipeline")
    sub = parser.add_subparsers(dest="mode", required=True)

    # file
    p_file = sub.add_parser("file", help="process existing wav file")
    p_file.add_argument("path")
    p_file.add_argument("--workers", type=int, default=4)
    p_file.add_argument("--model", type=str, default=DEFAULT_WHISPER_MODEL,
                        help="Whisper model name (override constants)")
    p_file.add_argument("--beam", type=int, default=DEFAULT_WHISPER_BEAM_SIZE,
                        help="Beam size for Whisper")

    # stream
    p_stream = sub.add_parser("stream", help="live stream from microphone")
    p_stream.add_argument("--chunk", type=float, default=4.0, help="seconds per chunk")
    p_stream.add_argument("--workers", type=int, default=2) 
    p_stream.add_argument("--record_secs", type=float, default=18.0,
                          help="total recording time in seconds (None for infinite)")
    p_stream.add_argument("--model", type=str, default=DEFAULT_WHISPER_MODEL)
    p_stream.add_argument("--beam", type=int, default=DEFAULT_WHISPER_BEAM_SIZE)

    # dir
    p_dir = sub.add_parser("dir", help="process all audio files in a directory")
    p_dir.add_argument("path")
    p_dir.add_argument("--workers", type=int, default=3)
    p_dir.add_argument("--truth_map", type=str, default="truth_map.txt")
    p_dir.add_argument("--model", type=str, default=DEFAULT_WHISPER_MODEL)
    p_dir.add_argument("--beam", type=int, default=DEFAULT_WHISPER_BEAM_SIZE)

    args = parser.parse_args()

    # ç”¨ CLI è¦†è“‹ ASR è¨­å®š
    # å¦‚æœå‘½ä»¤è¡Œ override æ¨¡å‹ï¼Œå°±é‡æ–°æ‹¿ä¸€ä¸ªæ–°çš„ asr
    asr = WhisperASR(model_name=args.model, gpu=use_gpu, beam=args.beam)

    if args.mode == "file":
        run_pipeline_file(args.path,
                          args.workers,
                          sep=sep, spk=spk, asr=asr)    
    elif args.mode == "stream":
        run_pipeline_stream(chunk_secs=args.chunk, max_workers=args.workers, sep=sep, spk=spk, asr=asr)
    elif args.mode == "dir":
        run_pipeline_dir(args.path, truth_map_path=args.truth_map, max_workers=args.workers, sep=sep, spk=spk, asr=asr)


if __name__ == "__main__":
    main()
