"""
===============================================================================
å³æ™‚éŒ„éŸ³èˆ‡èªéŸ³åˆ†é›¢æ¨¡çµ„ (Real-time Recording & Speech Separation Module)
===============================================================================

ç‰ˆæœ¬ï¼šv3.0.0
ä½œè€…ï¼šEvanLo62
æœ€å¾Œæ›´æ–°ï¼š2025-10-25

æ¨¡çµ„æ¦‚è¦ï¼š
-----------
æœ¬æ¨¡çµ„æä¾›å³æ™‚èªéŸ³åˆ†é›¢èˆ‡èªè€…è­˜åˆ¥è§£æ±ºæ–¹æ¡ˆï¼Œæ•´åˆå…ˆé€²çš„æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œ
å¯¦ç¾å¤šèªè€…èªéŸ³çš„ç²¾ç¢ºåˆ†é›¢èˆ‡å³æ™‚èº«ä»½è­˜åˆ¥ã€‚æ”¯æ´é‚ŠéŒ„éŸ³é‚Šè™•ç†çš„ä¸²æµæ¨¡å¼ï¼Œ
ç‚ºèªéŸ³æœƒè­°ã€å®¢æœç³»çµ±ã€èªéŸ³åŠ©ç†ç­‰æ‡‰ç”¨å ´æ™¯æä¾›å¼·å¤§çš„æŠ€è¡“æ”¯æ´ã€‚

ğŸ¯ æ ¸å¿ƒåŠŸèƒ½ï¼š
 â€¢ å³æ™‚èªéŸ³åˆ†é›¢ï¼šæ”¯æ´ 2-3 äººåŒæ™‚èªªè©±çš„èªéŸ³åˆ†é›¢
 â€¢ æ™ºæ…§èªè€…åµæ¸¬ï¼šè‡ªå‹•åµæ¸¬èªè€…æ•¸é‡ï¼Œå‹•æ…‹èª¿æ•´åˆ†é›¢ç­–ç•¥
 â€¢ éŸ³è¨Šå“è³ªå„ªåŒ–ï¼šå¤šå±¤é™å™ªèˆ‡éŸ³è³ªå¢å¼·è™•ç†
 â€¢ å½ˆæ€§éƒ¨ç½²æ¶æ§‹ï¼šæ”¯æ´ CPU/GPU æ··åˆé‹ç®—ï¼Œå¯æ“´å±•è‡³å¢é›†éƒ¨ç½²

ğŸ”§ æŠ€è¡“æ¶æ§‹ï¼š
-----------
 åˆ†é›¢å¼•æ“    ï¼šSpeechBrain SepFormer (16kHz å„ªåŒ–ç‰ˆæœ¬)
 éŸ³è¨Šè™•ç†    ï¼šPyTorch + torchaudio (CUDA åŠ é€Ÿ)
 ä¸¦ç™¼è™•ç†    ï¼šThreadPoolExecutor (å¤šåŸ·è¡Œç·’æœ€ä½³åŒ–)
 å“è³ªå¢å¼·    ï¼šé »è­œé–˜æ§é™å™ª + ç¶­ç´æ¿¾æ³¢ + å‹•æ…‹ç¯„åœå£“ç¸®

ğŸ“Š æ•ˆèƒ½æŒ‡æ¨™ï¼š
-----------
 â€¢ è™•ç†å»¶é²ï¼š< 500ms (å³æ™‚è™•ç†)
 â€¢ åˆ†é›¢ç²¾åº¦ï¼šSNR æå‡ 10-15dB
 â€¢ è­˜åˆ¥æº–ç¢ºç‡ï¼š> 95% (å·²çŸ¥èªè€…)
 â€¢ è¨˜æ†¶é«”ä½¿ç”¨ï¼š< 2GB (GPUæ¨¡å¼)
 â€¢ ä¸¦ç™¼èƒ½åŠ›ï¼šæ”¯æ´ 10+ åŒæ™‚æœƒè©±

ğŸš€ ä½¿ç”¨å ´æ™¯ï¼š
-----------
 âœ… å¤šäººèªéŸ³æœƒè­°è¨˜éŒ„èˆ‡åˆ†æ
 âœ… å®¢æœé›»è©±è‡ªå‹•åˆ†é›¢èˆ‡å“è³ªç›£æ§
 âœ… æ•™è‚²è¨“ç·´èªéŸ³å…§å®¹åˆ†æ
 âœ… åª’é«”è¨ªè«‡è‡ªå‹•è½‰éŒ„
 âœ… æ³•åº­è¨˜éŒ„èªè€…å€åˆ†

ğŸ”§ ç³»çµ±éœ€æ±‚ï¼š
-----------
 æœ€ä½é…ç½®ï¼š
  - Python 3.9+
  - RAM: 8GB+
  - å„²å­˜ç©ºé–“: 5GB+
  - ç¶²è·¯: ç©©å®šé€£ç·š (æ¨¡å‹ä¸‹è¼‰)

 å»ºè­°é…ç½®ï¼š
  - GPU: NVIDIA RTX 3060+ (8GB VRAM)
  - RAM: 16GB+
  - CPU: Intel i7 / AMD Ryzen 7+
  - SSD: 50GB+ å¯ç”¨ç©ºé–“

ğŸŒŸ é€²éšåŠŸèƒ½ï¼š
-----------
 â€¢ éŸ³è¨Šå“è³ªè©•ä¼°ï¼šSNR è‡ªå‹•åµæ¸¬èˆ‡é©æ‡‰æ€§è™•ç†
 â€¢ å‚™ç”¨åˆ†é›¢ç­–ç•¥ï¼šèªè€…åµæ¸¬å¤±æ•—æ™‚çš„æ™ºæ…§é™ç´šè™•ç†
 â€¢ å½ˆæ€§è¼¸å‡ºæ ¼å¼ï¼šæ”¯æ´æª”æ¡ˆå„²å­˜æˆ–è¨˜æ†¶é«”ä¸²æµ
 â€¢ æ•ˆèƒ½ç›£æ§ï¼šå³æ™‚çµ±è¨ˆè™•ç†æ•ˆç‡èˆ‡è³‡æºä½¿ç”¨

ğŸ“ æ ¸å¿ƒé¡åˆ¥ï¼š
-----------
 AudioSeparator     ï¼šä¸»è¦åˆ†é›¢å¼•æ“ï¼Œè² è²¬éŸ³è¨Šåˆ†é›¢èˆ‡å“è³ªè™•ç†
 SeparationModel    ï¼šæ¨¡å‹é…ç½®åˆ—èˆ‰ï¼Œæ”¯æ´ 2/3 äººåˆ†é›¢æ¨¡å‹

âš™ï¸ è¨­å®šåƒæ•¸ï¼š
-----------
 WINDOW_SIZE        = 6      # è™•ç†çª—å£ (ç§’)
 OVERLAP           = 0.5     # çª—å£é‡ç–Šç‡
 TARGET_RATE       = 16000   # ç›®æ¨™å–æ¨£ç‡
 THRESHOLD_NEW     = 0.385   # æ–°èªè€…åˆ¤å®šé–¾å€¼
 MIN_ENERGY        = 0.001   # æœ€å°éŸ³è¨Šèƒ½é‡é–¾å€¼

ğŸ“ˆ è¼¸å‡ºè³‡æ–™ï¼š
-----------
 åˆ†é›¢éŸ³æª”ï¼š./R3SI/Audio-storage/speaker{N}.wav
 æ··åˆéŸ³æª”ï¼š./R3SI/Audio-storage/mixed_audio_{timestamp}.wav
 è™•ç†æ—¥èªŒï¼šå³æ™‚è¼¸å‡ºè‡³ loggerï¼Œæ”¯æ´å¤šå±¤ç´šè¨˜éŒ„
 è­˜åˆ¥çµæœï¼šJSON æ ¼å¼ï¼ŒåŒ…å«èªè€…åç¨±ã€å„èªè€…éŸ³è¨Šã€æ™‚é–“æˆ³

ğŸ”— ç›¸é—œæ¨¡çµ„ï¼š
-----------
 â€¢ utils.logger (çµ±ä¸€æ—¥èªŒç®¡ç†)
 â€¢ utils.env_config (ç’°å¢ƒè®Šæ•¸é…ç½®)  
 â€¢ utils.constants (ç³»çµ±å¸¸æ•¸å®šç¾©)

ğŸ“š ä½¿ç”¨ç¯„ä¾‹ï¼š
-----------
 # å³æ™‚éŒ„éŸ³åˆ†é›¢
 separator = AudioSeparator(model_type=SeparationModel.SEPFORMER_3SPEAKER)
 separator.record_and_process("./output")
 
 # é›¢ç·šæª”æ¡ˆè™•ç†
 run_offline("meeting.wav", "./output", model_name="sepformer_3speaker")

ğŸ’¡ æœ€ä½³å¯¦è¸ï¼š
-----------
 1. ä½¿ç”¨ GPU åŠ é€Ÿä»¥ç²å¾—æœ€ä½³æ•ˆèƒ½
 2. å®šæœŸæ¸…ç†è¼¸å‡ºç›®éŒ„é¿å…å„²å­˜ç©ºé–“ä¸è¶³
 3. ç›£æ§ç³»çµ±è³‡æºä½¿ç”¨ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼
 4. åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­å•Ÿç”¨è©³ç´°æ—¥èªŒè¨˜éŒ„

ğŸ“ æŠ€è¡“æ”¯æ´ï¼š
-----------
 å°ˆæ¡ˆå€‰åº«ï¼šhttps://github.com/MISCapstoneProject/Unsaycret-API/tree/v0.4.2
 å•é¡Œå›å ±ï¼šGitHub Issues
 æŠ€è¡“æ–‡ä»¶ï¼šREADME.md & docs/

/*
 *                                                     __----~~~~~~~~~~~------___
 *                                    .  .   ~~//====......          __--~ ~~
 *                    -.            \_|//     |||\  ~~~~~~::::... /~
 *                 ___-==_       _-~o~  \/    |||  \            _/~~-
 *         __---~~~.==~||\=_    -_--~/_-~|-   |\   \        _/~
 *     _-~~     .=~    |  \-_    '-~7  /-   /  ||    \      /
 *   .~       .~       |   \ -_    /  /-   /   ||      \   /
 *  /  ____  /         |     \ ~-_/  /|- _/   .||       \ /
 *  |~~    ~~|--~~~~--_ \     ~==-/   | \~--===~~        .\
 *           '         ~-|      /|    |-~\~~       __--~~
 *                       |-~~-_/ |    |   ~\_   _-~            /\
 *                            /  \     \__   \/~                \__
 *                        _--~ _/ | .-~~____--~-/                  ~~==.
 *                       ((->/~   '.|||' -_|    ~~-/ ,              . _||
 *                                  -_     ~\      ~~---l__i__i__i--~~_/
 *                                  _-~-__   ~)  \--______________--~~
 *                                //.-~~~-~_--~- |-------~~~~~~~~
 *                                       //.-~~~--\
 *                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 * 
 *                               ç¥ç¸ä¿ä½‘            æ°¸ç„¡BUG
 */

===============================================================================
"""
from __future__ import annotations
import os
import numpy as np
import torch
import torchaudio
import pyaudio # type: ignore
import logging
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from speechbrain.inference import SepformerSeparation as separator
import noisereduce as nr # type: ignore
import threading
import time
from scipy import signal
from scipy.ndimage import uniform_filter1d
from enum import Enum

# å°å…¥èªè€…æ•¸é‡è­˜åˆ¥æ¨¡çµ„
from pyannote.audio import Pipeline

# å°å…¥æ—¥èªŒæ¨¡çµ„
from utils.logger import get_logger

# å°å…¥é…ç½® (ç’°å¢ƒè®Šæ•¸)
from utils.env_config import (
    AUDIO_RATE, FORCE_CPU, CUDA_DEVICE_INDEX, HF_ACCESS_TOKEN
)

# å°å…¥å¸¸æ•¸ (æ‡‰ç”¨ç¨‹å¼åƒæ•¸)  
from utils.constants import (
    DEFAULT_SEPARATION_MODEL,
    AUDIO_SAMPLE_RATE, AUDIO_CHUNK_SIZE, AUDIO_CHANNELS, 
    AUDIO_WINDOW_SIZE, AUDIO_OVERLAP, AUDIO_MIN_ENERGY_THRESHOLD, 
    AUDIO_MAX_BUFFER_MINUTES, API_MAX_WORKERS, AUDIO_TARGET_RATE
)

# å°å…¥å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨
from .dynamic_model_manager import (
    SeparationModel,
    MODEL_CONFIGS,
    create_dynamic_model_manager
)

# å°å…¥èªè€…è¨ˆæ•¸å™¨
from .speaker_counter import SpeakerCounter

# å°å…¥å–®äººé¸è·¯å™¨
from .best_speaker_selector import SingleSpeakerSelector

# å°å…¥è‡ªé©æ‡‰å¾Œé™å™ªæ¨¡çµ„
from .post_adaptive_denoiser import (
    AdaptiveDenoiser,
    get_adaptive_denoiser
)

# åŸºæœ¬éŒ„éŸ³åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
CHUNK = AUDIO_CHUNK_SIZE
FORMAT = pyaudio.paFloat32
CHANNELS = AUDIO_CHANNELS
RATE = AUDIO_RATE
TARGET_RATE = AUDIO_TARGET_RATE
WINDOW_SIZE = AUDIO_WINDOW_SIZE
OVERLAP = AUDIO_OVERLAP
DEVICE_INDEX = None  # ä½¿ç”¨é è¨­éŒ„éŸ³è¨­å‚™

# è™•ç†åƒæ•¸ï¼ˆå¾é…ç½®è®€å–ï¼‰
MIN_ENERGY_THRESHOLD = AUDIO_MIN_ENERGY_THRESHOLD
MAX_BUFFER_MINUTES = AUDIO_MAX_BUFFER_MINUTES

# éŸ³è¨Šè™•ç†åƒæ•¸
MIN_ENERGY_THRESHOLD = 0.001
NOISE_REDUCE_STRENGTH = 0.05  # é™ä½é™å™ªå¼·åº¦ä»¥ä¿æŒéŸ³è³ª
MAX_BUFFER_MINUTES = 5
SNR_THRESHOLD = 8  # é™ä½ SNR é–¾å€¼

# éŸ³è¨Šå“è³ªæ”¹å–„åƒæ•¸
WIENER_FILTER_STRENGTH = 0.01  # æ›´æº«å’Œçš„ç¶­ç´æ¿¾æ³¢
HIGH_FREQ_CUTOFF = 7500  # æé«˜é«˜é »æˆªæ­¢é»
DYNAMIC_RANGE_COMPRESSION = 0.7  # å‹•æ…‹ç¯„åœå£“ç¸®


DEFAULT_MODEL = DEFAULT_SEPARATION_MODEL

# ä¿®æ­£ DEFAULT_MODEL çš„è³¦å€¼
if DEFAULT_SEPARATION_MODEL == "tiger_2speaker":
    DEFAULT_MODEL = SeparationModel.TIGER_2SPEAKER
elif DEFAULT_SEPARATION_MODEL == "sepformer_2speaker":
    DEFAULT_MODEL = SeparationModel.SEPFORMER_2SPEAKER
elif DEFAULT_SEPARATION_MODEL == "sepformer_3speaker":
    DEFAULT_MODEL = SeparationModel.SEPFORMER_3SPEAKER
else:
    DEFAULT_MODEL = SeparationModel.TIGER_2SPEAKER  # é è¨­å€¼æ”¹ç‚ºæ‚¨çš„æ¨¡å‹

MODEL_NAME = MODEL_CONFIGS[DEFAULT_MODEL]["model_name"]
NUM_SPEAKERS = MODEL_CONFIGS[DEFAULT_MODEL]["num_speakers"]


# è¼¸å‡ºç›®éŒ„
OUTPUT_DIR = "R3SI/Audio-storage"  # å„²å­˜åˆ†é›¢å¾ŒéŸ³è¨Šçš„ç›®éŒ„
IDENTIFIED_DIR = "R3SI/Identified-Speakers"

# åˆå§‹åŒ–æ—¥èªŒç³»çµ±
logger = get_logger(__name__)

# ç¢ºä¿æ•´å€‹æ¨¡çµ„çš„ DEBUG è¨Šæ¯æœƒå°å‡ºï¼ˆå«æ‰€æœ‰ handlerï¼‰
# import logging
# logger.setLevel(logging.DEBUG)
# for h in logger.handlers:
#     try:
#         h.setLevel(logging.DEBUG)
#     except Exception:
#         pass

# åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ å…¨åŸŸå¿«å–
_GLOBAL_SEPARATOR_CACHE = {}
_GLOBAL_SPEAKER_PIPELINE_CACHE = None

# ================== èªè€…åˆ†é›¢é¡åˆ¥ ======================

class AudioSeparator:
    def __init__(self, model_type: SeparationModel = DEFAULT_MODEL, enable_dynamic_model=True, enable_post_denoiser=True):
        # è¨­å‚™é¸æ“‡é‚è¼¯ï¼šå„ªå…ˆè€ƒæ…® FORCE_CPU è¨­å®š
        if FORCE_CPU:
            self.device = "cpu"
            logger.info("ğŸ”§ FORCE_CPU=trueï¼Œå¼·åˆ¶ä½¿ç”¨ CPU é‹ç®—")
        else:
            if torch.cuda.is_available():
                # æª¢æŸ¥æŒ‡å®šçš„ CUDA è¨­å‚™æ˜¯å¦å­˜åœ¨
                if CUDA_DEVICE_INDEX < torch.cuda.device_count():
                    self.device = f"cuda:{CUDA_DEVICE_INDEX}"
                    # ç¢ºä¿è¨­å®šæ­£ç¢ºçš„è¨­å‚™
                    torch.cuda.set_device(CUDA_DEVICE_INDEX)
                    if CUDA_DEVICE_INDEX != 0:
                        logger.info(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„ CUDA è¨­å‚™: {CUDA_DEVICE_INDEX}")
                else:
                    logger.warning(f"âš ï¸  æŒ‡å®šçš„ CUDA è¨­å‚™ç´¢å¼• {CUDA_DEVICE_INDEX} ä¸å­˜åœ¨ï¼Œæ”¹ç”¨ cuda:0")
                    self.device = "cuda:0"
                    torch.cuda.set_device(0)
            else:
                self.device = "cpu"
                logger.info("ğŸ–¥ï¸  æœªåµæ¸¬åˆ° GPU è¨­å‚™ï¼Œä½¿ç”¨ CPU é‹ç®—")
                
        self.model_type = model_type
        self.model_config = MODEL_CONFIGS[model_type]
        self.num_speakers = self.model_config["num_speakers"]
        
        logger.info(f"ä½¿ç”¨è¨­å‚™: {self.device}")
        logger.info(f"æ¨¡å‹é¡å‹: {model_type.value}")
        logger.info(f"è¼‰å…¥æ¨¡å‹: {self.model_config['model_name']}")
        logger.info(f"æ”¯æ´èªè€…æ•¸é‡: {self.num_speakers}")
        
        # è¨­è¨ˆæ›´æº«å’Œçš„ä½é€šæ¿¾æ³¢å™¨
        nyquist = TARGET_RATE // 2
        cutoff = min(HIGH_FREQ_CUTOFF, nyquist - 100)
        self.lowpass_filter = signal.butter(2, cutoff / nyquist, btype='low', output='sos')
        
        # æ–°å¢å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨ç›¸é—œå±¬æ€§
        self.enable_dynamic_model = enable_dynamic_model
        
        if self.enable_dynamic_model:
            # ä½¿ç”¨å‹•æ…‹æ¨¡å‹ç®¡ç†å™¨
            self.model_manager = create_dynamic_model_manager(self.device)
            logger.info("å•Ÿç”¨å‹•æ…‹æ¨¡å‹é¸æ“‡æ©Ÿåˆ¶")
            
            # é è¼‰å…¥é è¨­æ¨¡å‹
            self.model_manager.preload_model(model_type)
            self.model, self.current_model_type = self.model_manager.get_model_for_speakers(self.num_speakers)
        else:
            # ä½¿ç”¨å›ºå®šæ¨¡å‹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
            self.model_manager = None
            self.current_model_type = model_type
            try:
                logger.info("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
                self.model = self._load_model()
                logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
                self._test_model()
            except Exception as e:
                logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                raise
        
        # å–®äººæƒ…å¢ƒï¼šè‡ªå‹•é¸è²é“å™¨ï¼ˆV2 åƒæ•¸ï¼‰
        self.single_selector = SingleSpeakerSelector(
            sr=TARGET_RATE,          # ä¾‹å¦‚ 16000ï¼Œè«‹ç¢ºä¿è·Ÿä½ è™•ç†éŸ³æª”çš„å¯¦éš›æ¡æ¨£ç‡ä¸€è‡´
            frame_ms=20,
            hop_ms=10,
            alpha=1.5,               # èƒ½é‡å¼ VAD é–€æª»ï¼ˆåƒ…ç”¨æ–¼çµ±è¨ˆ/ç‰¹å¾µçš„ gatingï¼‰
            min_rms=1e-6,

            # é€™å››å€‹æ˜¯æ–°ç‰ˆçš„æ¬Šé‡åç¨±
            w_sisdr=0.60,            # ä¸»ç‰¹å¾µï¼šå° mix çš„ SI-SDRï¼ˆæŠ•å½±åˆ†æ•¸ï¼‰
            w_band=0.25,             # 300â€“3400 Hz äººè²é »å¸¶èƒ½é‡ä½”æ¯”
            w_tonality=0.15,         # 1 - spectral flatness
            w_zcr_penalty=0.10,      # é›¶äº¤è¶Šç‡æ‡²ç½°ï¼ˆè¶Šé«˜è¶Šæ‡²ç½°ï¼‰

            tie_tol=0.02,
        )
        
        try:
            # åªæœ‰ç•¶æ¡æ¨£ç‡ä¸åŒæ™‚æ‰å»ºç«‹é‡æ¡æ¨£å™¨
            if RATE != TARGET_RATE:
                self.resampler = torchaudio.transforms.Resample(
                    orig_freq=RATE,
                    new_freq=TARGET_RATE
                ).to(self.device)
                logger.info(f"ğŸ”„ å»ºç«‹é‡æ¡æ¨£å™¨: {RATE}Hz â†’ {TARGET_RATE}Hz")
            else:
                self.resampler = None
                logger.info(f"âœ… æ¡æ¨£ç‡ä¸€è‡´ ({RATE}Hz)ï¼Œç„¡éœ€é‡æ¡æ¨£")
        except Exception as e:
            logger.error(f"é‡æ–°å–æ¨£å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
        
        self.executor = ThreadPoolExecutor(max_workers=API_MAX_WORKERS)
        self.futures = []
        self.is_recording = False
        self.output_files = []  # å„²å­˜åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘
        self.save_audio_files = True  # è¨­å®š: æ˜¯å¦å°‡åˆ†é›¢å¾Œçš„éŸ³è¨Šå„²å­˜ç‚ºwavæª”æ¡ˆ
        
        # æ™‚é–“è¿½è¹¤ç›¸é—œè®Šæ•¸
        self.session_start_time = None  # è¨˜éŒ„ session é–‹å§‹çš„çµ•å°æ™‚é–“
        self._current_t0 = 0.0  # ç´¯è¨ˆç›¸å°æ™‚é–“
        
        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'segments_processed': 0,
            'segments_skipped': 0,
            'errors': 0
        }
        
        self.max_buffer_size = int(RATE * MAX_BUFFER_MINUTES * 60 / CHUNK)

        # åˆå§‹åŒ–èªè€…è¨ˆæ•¸ç®¡ç·š
        self._init_speaker_count_pipeline()

        # æ”¹ç”¨ç¨ç«‹é¡åˆ¥é›†ä¸­ç®¡ç†èªè€…æ•¸é‡åµæ¸¬ï¼Œä¸¦å‚³å…¥å¿«å–çš„ç®¡ç·š
        self.spk_counter = SpeakerCounter(
            hf_token=HF_ACCESS_TOKEN, 
            device=self.device, 
            pipeline=getattr(self, 'speaker_count_pipeline', None),
            logger=logger
        )
        logger.info("èªè€…è¨ˆæ•¸å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–è‡ªé©æ‡‰é™å™ªå™¨
        self.enable_post_denoiser = enable_post_denoiser
        if self.enable_post_denoiser:
            self.denoiser = get_adaptive_denoiser(
                device=self.device,
                aggressive_mode=False,         # æ¨™æº–æ¨¡å¼
                enable_hiss_removal=True,      # å»é™¤å˜¶å˜¶è²
                enable_rumble_filter=True,     # å»é™¤ä½é »
                enable_dereverb=False,         # å¯é¸ï¼šå»æ··éŸ¿ï¼ˆè¼ƒæ…¢ï¼‰
                hiss_threshold=0.15            # æ•æ„Ÿåº¦
            )
            logger.info("è‡ªé©æ‡‰é™å™ªå™¨åˆå§‹åŒ–å®Œæˆ")
        
        self._last_single_route_idx = None
        self._last_single_route_score = None
        
        logger.info("AudioSeparator åˆå§‹åŒ–å®Œæˆ")

    def _init_speaker_count_pipeline(self):
        """åˆå§‹åŒ–èªè€…è¨ˆæ•¸ç®¡ç·š - ä½¿ç”¨å…¨åŸŸå¿«å–"""
        global _GLOBAL_SPEAKER_PIPELINE_CACHE
        
        try:
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰å…¨åŸŸå¿«å–çš„ç®¡ç·š
            if _GLOBAL_SPEAKER_PIPELINE_CACHE is not None:
                self.speaker_count_pipeline = _GLOBAL_SPEAKER_PIPELINE_CACHE
                logger.info("ä½¿ç”¨å¿«å–çš„èªè€…è¨ˆæ•¸ç®¡ç·š")
                return
            
            if HF_ACCESS_TOKEN:
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1", 
                    use_auth_token=HF_ACCESS_TOKEN
                )
                # å°‡ç®¡ç·šç§»åˆ°ç›¸åŒè¨­å‚™
                if hasattr(self, 'device'):
                    pipeline.to(torch.device(self.device))
                
                # å¿«å–åˆ°å…¨åŸŸè®Šæ•¸
                _GLOBAL_SPEAKER_PIPELINE_CACHE = pipeline
                self.speaker_count_pipeline = pipeline
                logger.info("èªè€…è¨ˆæ•¸ç®¡ç·šè¼‰å…¥ä¸¦å¿«å–æˆåŠŸ")
            else:
                logger.warning("æœªæä¾› HF_ACCESS_TOKENï¼Œèªè€…è¨ˆæ•¸åŠŸèƒ½å°‡å—é™")
                self.speaker_count_pipeline = None
        except Exception as e:
            logger.warning(f"èªè€…è¨ˆæ•¸ç®¡ç·šè¼‰å…¥å¤±æ•—: {e}")
            self.speaker_count_pipeline = None

    def _load_model(self):
        """è¼‰å…¥èªè€…åˆ†é›¢æ¨¡å‹"""
        model_name = self.model_config["model_name"]
        
        # ä½¿ç”¨ SpeechBrain SepFormer æ¨¡å‹
        try:
            logger.info(f"è¼‰å…¥ SpeechBrain æ¨¡å‹: {model_name}")
            
            # æª¢æŸ¥æœ¬åœ°æ¨¡å‹ç›®éŒ„æ˜¯å¦åŒ…å«ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ
            local_model_path = os.path.abspath(f"models/{self.model_type.value}")
            if os.path.exists(local_model_path):
                logger.info(f"æª¢æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾‘: {local_model_path}")
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµ (Windows JUNCTION æŒ‡å‘ Linux è·¯å¾‘)
                hyperparams_file = os.path.join(local_model_path, "hyperparams.yaml")
                if os.path.exists(hyperparams_file):
                    try:
                        # æ¸¬è©¦æª”æ¡ˆè®€å–æ¬Šé™
                        with open(hyperparams_file, 'r', encoding='utf-8') as f:
                            content = f.read(100)  # è®€å–å‰100å€‹å­—ç¬¦ä¾†æ¸¬è©¦
                        logger.info("æœ¬åœ°æ¨¡å‹æª”æ¡ˆè®€å–æ­£å¸¸")
                    except (PermissionError, OSError, UnicodeDecodeError) as e:
                        logger.warning(f"æœ¬åœ°æ¨¡å‹æª”æ¡ˆç„¡æ³•è®€å–: {e}")
                        logger.info("åµæ¸¬åˆ°ç„¡æ•ˆçš„ç¬¦è™Ÿé€£çµï¼Œæº–å‚™é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                        
                        # åˆªé™¤åŒ…å«ç„¡æ•ˆç¬¦è™Ÿé€£çµçš„ç›®éŒ„
                        try:
                            import shutil
                            shutil.rmtree(local_model_path, ignore_errors=True)
                            logger.info(f"å·²åˆªé™¤ç„¡æ•ˆçš„æ¨¡å‹ç›®éŒ„: {local_model_path}")
                        except Exception as rm_error:
                            logger.warning(f"åˆªé™¤æ¨¡å‹ç›®éŒ„æ™‚ç™¼ç”ŸéŒ¯èª¤: {rm_error}")
            
            # å˜—è©¦è¼‰å…¥æ¨¡å‹ (å¦‚æœæœ¬åœ°æª”æ¡ˆç„¡æ•ˆï¼ŒSpeechBrain æœƒè‡ªå‹•é‡æ–°ä¸‹è¼‰)
            model = separator.from_hparams(
                source=model_name,
                savedir=os.path.abspath(f"models/{self.model_type.value}"),
                run_opts={"device": self.device}
            )
            logger.info("SpeechBrain æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return model
            
        except Exception as e:
            logger.error(f"SpeechBrain æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            
            # æœ€å¾Œå˜—è©¦ï¼šå¼·åˆ¶é‡æ–°ä¸‹è¼‰
            try:
                logger.info("å˜—è©¦å¼·åˆ¶é‡æ–°ä¸‹è¼‰æ¨¡å‹...")
                import shutil
                local_model_path = os.path.abspath(f"models/{self.model_type.value}")
                if os.path.exists(local_model_path):
                    shutil.rmtree(local_model_path, ignore_errors=True)
                    logger.info("å·²æ¸…é™¤æœ¬åœ°æ¨¡å‹å¿«å–")
                
                model = separator.from_hparams(
                    source=model_name,
                    savedir=os.path.abspath(f"models/{self.model_type.value}"),
                    run_opts={"device": self.device}
                )
                logger.info("å¼·åˆ¶é‡æ–°ä¸‹è¼‰å¾Œï¼Œæ¨¡å‹è¼‰å…¥æˆåŠŸ")
                return model
                
            except Exception as final_error:
                logger.error(f"æ‰€æœ‰è¼‰å…¥å˜—è©¦å‡å¤±æ•—: {final_error}")
                raise Exception(f"æ¨¡å‹è¼‰å…¥å®Œå…¨å¤±æ•—ã€‚è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šå’Œæ¨¡å‹å¯ç”¨æ€§ã€‚åŸå§‹éŒ¯èª¤: {e}")

    def _test_model(self):
        """æ¸¬è©¦æ¨¡å‹"""
        try:
            with torch.no_grad():
                # SpeechBrain SepFormer æ¨¡å‹æ¸¬è©¦
                # SepFormer æœŸæœ›è¼¸å…¥æ ¼å¼ç‚º [batch, samples]
                test_audio = torch.randn(1, AUDIO_SAMPLE_RATE).to(self.device)
                logger.debug(f"SpeechBrain æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape}")
                output = self.model.separate_batch(test_audio)
                    
            logger.info("æ¨¡å‹æ¸¬è©¦é€šé")
            logger.debug(f"è¼¸å‡ºå½¢ç‹€: {output.shape if hasattr(output, 'shape') else type(output)}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
            logger.error(f"æ¸¬è©¦éŸ³è¨Šå½¢ç‹€: {test_audio.shape if 'test_audio' in locals() else 'N/A'}")
            raise
        
    def _infer_layout(self, est: torch.Tensor) -> tuple[str, int, int]:
        """
        å›å‚³ (layout, spk_axis, time_axis)
        layout âˆˆ {'BST','BTS'}ï¼›BST è¡¨ç¤º [B, S, T]ã€BTS è¡¨ç¤º [B, T, S]
        """
        assert est.dim() == 3, f"unexpected est shape: {tuple(est.shape)}"
        B, D1, D2 = est.shape[0], est.shape[1], est.shape[2]
        # å“ªå€‹ç¶­åº¦åƒã€Œèªªè©±è€…ã€? ï¼ˆå¾ˆå°ä¸”åœ¨ 1-4 ä¹‹é–“ï¼‰
        if 1 <= D1 <= 4 and not (1 <= D2 <= 4):
            return "BST", 1, 2  # [B, S, T]
        if 1 <= D2 <= 4 and not (1 <= D1 <= 4):
            return "BTS", 2, 1  # [B, T, S]
        # éƒ½åƒæˆ–éƒ½ä¸åƒï¼šåå¥½ [B, S, T]
        return "BST", 1, 2

    def _normalize_estimates(self, est: torch.Tensor) -> tuple[torch.Tensor, str, int, int]:
        """
        å°æ¯å€‹èªªè©±è€…ã€Œæ²¿æ™‚é–“è»¸ã€åš peak normalizeï¼ˆå¸¸æ•¸ç¸®æ”¾ï¼‰ï¼Œé¿å…æ™‚é–“é»ä¾è³´çš„å¤±çœŸã€‚
        å›å‚³ (normalized, layout, spk_axis, time_axis)
        """
        if est.dim() == 2:
            peak = est.abs().amax(dim=-1, keepdim=True).clamp_min(1e-8)
            return est / peak, "BT", -1, -1
        layout, s_ax, t_ax = self._infer_layout(est)
        peak = est.abs().amax(dim=t_ax, keepdim=True).clamp_min(1e-8)
        return est / peak, layout, s_ax, t_ax

    def _normalize_audio_energy(
        self, 
        audio: torch.Tensor, 
        segment_index: int,
        target_peak: float = 0.7,
        min_peak_threshold: float = 0.05,
        noise_floor: float = 0.001
    ) -> torch.Tensor | None:
        """
        æ­£è¦åŒ–éŸ³è¨Šèƒ½é‡ï¼Œç¢ºä¿è¶³å¤ çš„è¨Šè™Ÿå¼·åº¦ä¾›å¾ŒçºŒè™•ç†ã€‚
        
        ç­–ç•¥ï¼š
        1. å¦‚æœå³°å€¼ < min_peak_thresholdï¼šæ­£è¦åŒ–åˆ° target_peak
        2. å¦‚æœå³°å€¼ >= min_peak_thresholdï¼šä¿æŒåŸæ¨£ï¼ˆå·²ç¶“å¤ å¤§è²ï¼‰
        3. å¦‚æœæ•´é«”èƒ½é‡éä½ï¼ˆ< noise_floorï¼‰ï¼šè¦–ç‚ºç´”å™ªéŸ³ï¼Œè¿”å› None
        
        Args:
            audio: è¼¸å…¥éŸ³è¨Šå¼µé‡ [C, T] æˆ– [B, C, T]
            segment_index: ç‰‡æ®µç´¢å¼•ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
            target_peak: ç›®æ¨™å³°å€¼ï¼ˆé è¨­ 0.7ï¼Œç‚ºæ­£å¸¸å°è©±éŸ³é‡ï¼‰
            min_peak_threshold: ä½æ–¼æ­¤å€¼æ‰éœ€è¦æ­£è¦åŒ–ï¼ˆé è¨­ 0.05ï¼‰
            noise_floor: å™ªéŸ³åº•é™ï¼Œä½æ–¼æ­¤å€¼è¦–ç‚ºç„¡æ•ˆéŸ³è¨Šï¼ˆé è¨­ 0.001ï¼‰
        
        Returns:
            æ­£è¦åŒ–å¾Œçš„éŸ³è¨Šå¼µé‡ï¼Œæˆ– Noneï¼ˆå¦‚æœéŸ³è¨Šç„¡æ•ˆï¼‰
        """
        if audio is None or audio.numel() == 0:
            return None
        
        # è¨ˆç®—ç•¶å‰å³°å€¼
        current_peak = float(audio.abs().max())
        
        # è¨ˆç®— RMS èƒ½é‡
        current_rms = float(audio.pow(2).mean().sqrt())
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”å™ªéŸ³æˆ–éœéŸ³
        if current_rms < noise_floor:
            logger.debug(
                f"ç‰‡æ®µ {segment_index} - éŸ³è¨Šèƒ½é‡éä½ "
                f"(RMS={current_rms:.6f} < {noise_floor})ï¼Œè¦–ç‚ºå™ªéŸ³"
            )
            return None
        
        # åˆ¤æ–·æ˜¯å¦éœ€è¦æ­£è¦åŒ–
        if current_peak < min_peak_threshold:
            # è¨ˆç®—å¢ç›Šä¿‚æ•¸
            gain = target_peak / max(current_peak, 1e-8)
            
            # é™åˆ¶æœ€å¤§å¢ç›Šï¼ˆé¿å…éåº¦æ”¾å¤§å™ªéŸ³ï¼‰
            max_gain = 20.0  # æœ€å¤šæ”¾å¤§ 20 å€ï¼ˆç´„ 26dBï¼‰
            if gain > max_gain:
                logger.warning(
                    f"ç‰‡æ®µ {segment_index} - è¨ˆç®—å¢ç›Šéå¤§ ({gain:.1f}x)ï¼Œ"
                    f"é™åˆ¶ç‚º {max_gain}x"
                )
                gain = max_gain
            
            # æ‡‰ç”¨å¢ç›Š
            normalized = audio * gain
            
            # é˜²æ­¢å‰Šæ³¢ï¼ˆclippingï¼‰
            normalized_peak = float(normalized.abs().max())
            if normalized_peak > 1.0:
                # å¦‚æœè¶…é 1.0ï¼Œç¸®æ¸›åˆ° 0.95
                normalized = normalized * (0.95 / normalized_peak)
                logger.debug(
                    f"ç‰‡æ®µ {segment_index} - åµæ¸¬åˆ°å‰Šæ³¢ï¼Œèª¿æ•´å³°å€¼åˆ° 0.95"
                )
            
            logger.info(
                f"ç‰‡æ®µ {segment_index} - éŸ³è¨Šæ­£è¦åŒ–: "
                f"peak {current_peak:.4f}â†’{float(normalized.abs().max()):.4f} "
                f"(å¢ç›Š {gain:.2f}x)"
            )
            
            # ç¢ºä¿è¿”å›çš„å¼µé‡ä¿æŒåœ¨åŸå§‹è¨­å‚™ä¸Š
            return normalized.to(audio.device)
        else:
            # å³°å€¼å·²ç¶“è¶³å¤ ï¼Œä¸éœ€è¦æ­£è¦åŒ–
            logger.debug(
                f"ç‰‡æ®µ {segment_index} - éŸ³è¨Šå³°å€¼æ­£å¸¸ "
                f"(peak={current_peak:.4f})ï¼Œè·³éæ­£è¦åŒ–"
            )
            return audio
    
    def _get_appropriate_model(self, num_speakers: int) -> tuple[separator, SeparationModel]:
        """
        å–å¾—é©ç•¶çš„æ¨¡å‹å¯¦ä¾‹
        
        Args:
            num_speakers: åµæ¸¬åˆ°çš„èªè€…æ•¸é‡
            
        Returns:
            tuple: (æ¨¡å‹å¯¦ä¾‹, æ¨¡å‹é¡å‹)
        """
        if self.enable_dynamic_model and self.model_manager:
            return self.model_manager.get_model_for_speakers(num_speakers)
        else:
            # å›ºå®šæ¨¡å‹æ¨¡å¼
            return self.model, self.current_model_type

    def process_audio(self, audio_data: np.ndarray) -> torch.Tensor:
        """è™•ç†éŸ³è¨Šæ ¼å¼ï¼šå°‡åŸå§‹éŒ„éŸ³è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼"""
        try:
            # è½‰æ›ç‚º float32
            if FORMAT == pyaudio.paInt16:
                audio_float = audio_data.astype(np.float32) / 32768.0
            else:
                audio_float = audio_data.astype(np.float32)
            
            # èƒ½é‡æª¢æ¸¬ï¼šéä½å‰‡ç•¥é
            energy = np.mean(np.abs(audio_float))
            if energy < MIN_ENERGY_THRESHOLD:
                logger.debug(f"éŸ³è¨Šèƒ½é‡ ({energy:.6f}) ä½æ–¼é–¾å€¼ ({MIN_ENERGY_THRESHOLD})")
                return None
            
            # é‡å¡‘ç‚ºæ­£ç¢ºå½¢ç‹€
            if len(audio_float.shape) == 1:
                audio_float = audio_float.reshape(-1, CHANNELS)

            # èª¿æ•´å½¢ç‹€ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥ï¼š[channels, time]
            audio_tensor = torch.from_numpy(audio_float).T.float()

            # å¦‚æœæ˜¯é›™è²é“è€Œæ¨¡å‹åªæ”¯æ´å–®è²é“å‰‡å–å¹³å‡
            if audio_tensor.shape[0] == 2:
                audio_tensor = torch.mean(audio_tensor, dim=0, keepdim=True)

            # ç§»è‡³ GPU ä¸¦æ¢ä»¶å¼é‡æ¡æ¨£
            audio_tensor = audio_tensor.to(self.device)
            if self.resampler is not None:
                resampled = self.resampler(audio_tensor)
            else:
                resampled = audio_tensor
            
            # ç¢ºä¿å½¢ç‹€æ­£ç¢º
            if len(resampled.shape) == 1:
                resampled = resampled.unsqueeze(0)
            
            return resampled
            
        except Exception as e:
            logger.error(f"éŸ³è¨Šè™•ç†éŒ¯èª¤ï¼š{e}")
            self.processing_stats['errors'] += 1
            return None

    def cleanup_futures(self):
        """æ¸…ç†å·²å®Œæˆçš„ä»»å‹™"""
        completed_futures = []
        for future in self.futures:
            if future.done():
                try:
                    future.result()  # ç²å–çµæœä»¥æ•ç²ä»»ä½•ç•°å¸¸
                except Exception as e:
                    logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                    self.processing_stats['errors'] += 1
                completed_futures.append(future)
        
        # ç§»é™¤å·²å®Œæˆçš„ä»»å‹™
        for future in completed_futures:
            self.futures.remove(future)

    def record_and_process(self, output_dir):
        """éŒ„éŸ³ä¸¦è™•ç†éŸ³è¨Šçš„ä¸»è¦æ–¹æ³•"""
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šçš„ç·©è¡å€
        mixed_audio_buffer = []
        
        try:
            # æ­¥é©Ÿ0: åˆå§‹åŒ–èªè€…è­˜åˆ¥å™¨
            # identifier = SpeakerIdentifier()

            # æ­¥é©Ÿ1: åˆå§‹åŒ–éŒ„éŸ³è£ç½®
            p = pyaudio.PyAudio()
            
            # æª¢æŸ¥è¨­å‚™å¯ç”¨æ€§
            if DEVICE_INDEX is not None:
                device_info = p.get_device_info_by_index(DEVICE_INDEX)
                logger.info(f"ä½¿ç”¨éŸ³è¨Šè¨­å‚™: {device_info['name']}")
            
            stream = p.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK,
                input_device_index=DEVICE_INDEX
            )
            
            logger.info("é–‹å§‹éŒ„éŸ³...")
            
            # æ­¥é©Ÿ2: è¨ˆç®—ç·©è¡å€åƒæ•¸
            samples_per_window = int(WINDOW_SIZE * RATE)
            window_frames = int(samples_per_window / CHUNK)
            overlap_frames = int((OVERLAP * RATE) / CHUNK)
            slide_frames = window_frames - overlap_frames
            
            # åˆå§‹åŒ–è™•ç†è®Šæ•¸
            buffer = []
            segment_index = 0
            self.is_recording = True
            last_stats_time = time.time()
            
            # æ­¥é©Ÿ3: éŒ„éŸ³ä¸»å¾ªç’°
            while self.is_recording:
                try:
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    frame = np.frombuffer(data, dtype=np.float32 if FORMAT == pyaudio.paFloat32 else np.int16)
                    buffer.append(frame)
                    
                    # é™åˆ¶ mixed_audio_buffer å¤§å°ä»¥é˜²æ­¢è¨˜æ†¶é«”è€—ç›¡
                    mixed_audio_buffer.append(frame.copy())
                    if len(mixed_audio_buffer) > self.max_buffer_size:
                        mixed_audio_buffer.pop(0)
                        
                except IOError as e:
                    logger.warning(f"éŒ„éŸ³æ™‚ç™¼ç”ŸIOéŒ¯èª¤ï¼š{e}")
                    continue
                
                # æ­¥é©Ÿ4: ç•¶ç´¯ç©è¶³å¤ è³‡æ–™æ™‚é€²è¡Œè™•ç†
                if len(buffer) >= window_frames:
                    segment_index += 1
                    audio_data = np.concatenate(buffer[:window_frames])
                    audio_tensor = self.process_audio(audio_data)
                    
                    # æ­¥é©Ÿ5: å¦‚æœéŸ³è¨Šæœ‰æ•ˆï¼Œå•Ÿå‹•èªè€…åˆ†é›¢è™•ç†
                    if audio_tensor is not None:
                        logger.info(f"è™•ç†ç‰‡æ®µ {segment_index}")
                        future = self.executor.submit(
                            self.separate_and_save,
                            audio_tensor,
                            output_dir,
                            segment_index
                        )
                        self.futures.append(future)
                        self.processing_stats['segments_processed'] += 1
                    else:
                        self.processing_stats['segments_skipped'] += 1
                    
                    # æ­¥é©Ÿ6: ç§»å‹•è¦–çª—
                    buffer = buffer[slide_frames:]
                    
                    # å®šæœŸæ¸…ç†å·²å®Œæˆçš„ä»»å‹™
                    if segment_index % 10 == 0:
                        self.cleanup_futures()
                    
                    # æ¯30ç§’å ±å‘Šä¸€æ¬¡çµ±è¨ˆè³‡è¨Š
                    current_time = time.time()
                    if current_time - last_stats_time > 30:
                        self._log_statistics()
                        last_stats_time = current_time
                    
        except Exception as e:
            logger.error(f"éŒ„éŸ³éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        finally:
            # æ­¥é©Ÿ7: æ¸…ç†è³‡æº
            self._cleanup_resources(p, stream, mixed_audio_buffer, output_dir)

    def _cleanup_resources(self, p, stream, mixed_audio_buffer, output_dir):
        """æ¸…ç†è³‡æº"""
        # åœæ­¢ä¸¦é—œé–‰éŸ³è¨Šæµ
        if stream is not None:
            try:
                stream.stop_stream()
                stream.close()
                logger.info("éŸ³è¨Šæµå·²é—œé–‰")
            except Exception as e:
                logger.error(f"é—œé–‰éŸ³è¨Šæµæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        if p is not None:
            try:
                p.terminate()
                logger.info("PyAudio å·²çµ‚æ­¢")
            except Exception as e:
                logger.error(f"çµ‚æ­¢ PyAudio æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # ç­‰å¾…æ‰€æœ‰è™•ç†ä»»å‹™å®Œæˆ
        logger.info("ç­‰å¾…è™•ç†ä»»å‹™å®Œæˆ...")
        for future in self.futures:
            try:
                future.result(timeout=15.0)
            except Exception as e:
                logger.error(f"è™•ç†ä»»å‹™ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        self.executor.shutdown(wait=True)
        logger.info("ç·šç¨‹æ± å·²é—œé–‰")
        
        # å„²å­˜åŸå§‹æ··åˆéŸ³è¨Š
        self._save_mixed_audio(mixed_audio_buffer, output_dir)
        
        # è¨˜éŒ„æœ€çµ‚çµ±è¨ˆ
        self._log_final_statistics()
        
        # æ¸…ç†æ¨¡å‹ç®¡ç†å™¨
        if self.model_manager:
            self.model_manager.cleanup()
        
        # æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·š
        if hasattr(self, 'speaker_count_pipeline') and self.speaker_count_pipeline is not None:
            try:
                # æ¸…ç†ç®¡ç·šè³‡æº
                del self.speaker_count_pipeline
                self.speaker_count_pipeline = None
                logger.info("èªè€…è¨ˆæ•¸ç®¡ç·šå·²æ¸…ç†")
            except Exception as e:
                logger.error(f"æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        
        # æ¸…ç†GPUè¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        logger.info("éŒ„éŸ³çµæŸï¼Œæ‰€æœ‰è³‡æºå·²æ¸…ç†")

    def _save_mixed_audio(self, mixed_audio_buffer, output_dir):
        """å„²å­˜æ··åˆéŸ³è¨Š"""
        if not mixed_audio_buffer:
            return ""
            
        try:
            mixed_audio = np.concatenate(mixed_audio_buffer)
            mixed_audio = mixed_audio.reshape(-1, CHANNELS)
            
            timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')
            mixed_output_file = os.path.join(
                output_dir,
                f"mixed_audio_{timestamp}.wav"
            )
            
            mixed_tensor = torch.from_numpy(mixed_audio).T.float()
            torchaudio.save(
                mixed_output_file,
                mixed_tensor,
                RATE
            )
            logger.info(f"å·²å„²å­˜åŸå§‹æ··åˆéŸ³è¨Šï¼š{mixed_output_file}")
            return mixed_output_file
            
        except Exception as e:
            logger.error(f"å„²å­˜æ··åˆéŸ³è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return ""

    def _log_statistics(self):
        """è¨˜éŒ„çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        logger.info(f"çµ±è¨ˆ - å·²è™•ç†: {stats['segments_processed']}, "
                   f"å·²è·³é: {stats['segments_skipped']}, "
                   f"éŒ¯èª¤: {stats['errors']}, "
                   f"é€²è¡Œä¸­ä»»å‹™: {len(self.futures)}")

    def _log_final_statistics(self):
        """è¨˜éŒ„æœ€çµ‚çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats
        total = stats['segments_processed'] + stats['segments_skipped']
        if total > 0:
            success_rate = (stats['segments_processed'] / total) * 100
            logger.info(f"æœ€çµ‚çµ±è¨ˆ - ç¸½ç‰‡æ®µ: {total}, "
                       f"æˆåŠŸè™•ç†: {stats['segments_processed']} ({success_rate:.1f}%), "
                       f"è·³é: {stats['segments_skipped']}, "
                       f"éŒ¯èª¤: {stats['errors']}")

    def separate_and_save(self, audio_tensor, output_dir, segment_index, absolute_start_time=None):
        """
        åˆ†é›¢ä¸¦å„²å­˜éŸ³è¨Šï¼ˆåƒ…ä¿ç•™åŸå§‹åˆ†é›¢çµæœï¼Œä¸åšä»»ä½•å¾Œè™•ç†ï¼‰
        æµç¨‹ï¼šèªè€…è¨ˆæ•¸ â†’ å‹•æ…‹æ¨¡å‹é¸æ“‡ â†’ åˆ†é›¢ â†’ å„²å­˜
        """
        try:
            # éŸ³è¨Šæ­£è¦åŒ–
            # audio_tensor = self._normalize_audio_energy(audio_tensor, segment_index)
            # if audio_tensor is None:
            #     logger.warning(f"ç‰‡æ®µ {segment_index} - éŸ³è¨Šæ­£è¦åŒ–å¤±æ•—ï¼Œè·³é")
            #     return []
            audio_tensor = self.denoiser.denoise(
                audio_tensor,
                sample_rate=TARGET_RATE
            ) if self.enable_post_denoiser else audio_tensor
            
            # 1) èªè€…æ•¸é‡åµæ¸¬ï¼ˆç¶­æŒåŸé‚è¼¯ï¼‰
            detected_speakers = self.spk_counter.count_with_refine(
                audio=audio_tensor,
                sample_rate=TARGET_RATE,
                expected_min=1,
                expected_max=3,
                first_pass_range=(1, 3),
                allow_zero=True,
                debug=False,
            )
            logger.info(f"ç‰‡æ®µ {segment_index} - åµæ¸¬åˆ° {detected_speakers} ä½èªªè©±è€…")

            if detected_speakers == 0:
                ok, m = self.spk_counter._has_voice(audio_tensor, TARGET_RATE, return_metrics=True)
                
                # ä½¿ç”¨æ¨™æº–é–¾å€¼
                voiced_ratio_thresh = 0.12
                voiced_union_thresh = 0.50
                loud_frac_thresh = 0.05
                
                strong_voice = ok and \
                            (m["voiced_ratio"] >= voiced_ratio_thresh) and \
                            (m["voiced_union"] >= voiced_union_thresh) and \
                            (m.get("loud_frac", 0.0) >= loud_frac_thresh)
                
                if not strong_voice:
                    logger.info(
                        f"ç‰‡æ®µ {segment_index} - ç„¡èªéŸ³/éçŸ­ï¼ˆratio={m['voiced_ratio']:.3f}, "
                        f"union={m['voiced_union']:.2f}s, loud={m.get('loud_frac',0.0):.3f}, "
                        f"thresh=[{voiced_ratio_thresh:.2f}, {voiced_union_thresh:.2f}, {loud_frac_thresh:.2f}]ï¼‰ï¼Œè·³é"
                    )
                    return []
                logger.warning(f"ç‰‡æ®µ {segment_index} - ç¬¬ä¸€æ¬¡åµæ¸¬ 0ï¼Œä½†èªéŸ³è·¡è±¡åå¼·ï¼Œå˜—è©¦ 1â€“2 äººé‡è©¦")
                retry = self.spk_counter.count_with_refine(
                    audio=audio_tensor,
                    sample_rate=TARGET_RATE,
                    expected_min=1,
                    expected_max=3,
                    first_pass_range=(1, 3),
                    allow_zero=False,
                    debug=False,
                )
                detected_speakers = max(1, int(retry))

            # 2) å‹•æ…‹é¸æ“‡æ¨¡å‹ï¼ˆç¶­æŒåŸé‚è¼¯ï¼‰
            current_model, current_model_type = self._get_appropriate_model(detected_speakers)
            model_config = self.model_manager.get_model_config(current_model_type) if self.model_manager else MODEL_CONFIGS[current_model_type]
            logger.debug(f"ä½¿ç”¨æ¨¡å‹: {current_model_type.value} (åµæ¸¬èªè€…: {detected_speakers})")

            # 3) æ™‚é–“æˆ³è™•ç†ï¼ˆç¶­æŒåŸé‚è¼¯ï¼‰
            if absolute_start_time is None:
                from datetime import timezone, timedelta
                taipei_tz = timezone(timedelta(hours=8))
                absolute_start_time = datetime.now(taipei_tz)
            current_t0 = getattr(self, "_current_t0", 0.0)
            seg_duration = audio_tensor.shape[-1] / TARGET_RATE
            results = []

            with torch.no_grad():
                # 4) è¼¸å…¥æ•´ç†æˆ [batch, samples]ï¼Œä¸¦ç¢ºä¿åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                if len(audio_tensor.shape) == 3 and audio_tensor.shape[1] == 1:
                    audio_tensor = audio_tensor.squeeze(1)
                
                # ç¢ºä¿ audio_tensor åœ¨æ¨¡å‹è¨­å‚™ä¸Š
                audio_tensor = audio_tensor.to(self.device)

                # 5) åšã€ŒåŸå§‹åˆ†é›¢ã€
                if current_model_type == SeparationModel.TIGER_2SPEAKER:
                    # TIGER æœŸæœ›è¼¸å…¥ç‚º [B, C, T] æˆ– [B, T]
                    if audio_tensor.dim() == 2:  # [B, T]
                        audio_tensor_input = audio_tensor.unsqueeze(1)  # [B, 1, T]
                    else:
                        audio_tensor_input = audio_tensor
                    
                    # ç¢ºä¿è¼¸å…¥åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                    audio_tensor_input = audio_tensor_input.to(self.device)
                    
                    logger.debug(f"Tiger è¼¸å…¥å½¢ç‹€: {audio_tensor_input.shape}, è¨­å‚™: {audio_tensor_input.device}")
                    separated = current_model(audio_tensor_input)
                    logger.debug(f"Tiger åŸå§‹è¼¸å‡ºå½¢ç‹€: {separated.shape}, è¨­å‚™: {separated.device}")
                    
                    # Tiger æ¨¡å‹å¯èƒ½è¼¸å‡º [B, S, T] æ ¼å¼ï¼Œéœ€è¦ç¢ºä¿æ­£ç¢ºè™•ç†
                    if separated.dim() == 2:
                        # å¦‚æœæ˜¯ [B, T]ï¼Œè½‰æ›ç‚º [B, 1, T]
                        separated = separated.unsqueeze(1)
                        logger.debug(f"Tiger è¼¸å‡ºèª¿æ•´å¾Œå½¢ç‹€: {separated.shape}")
                    elif separated.dim() == 3:
                        # ç¢ºèªæ˜¯ [B, S, T] é‚„æ˜¯ [B, T, S]
                        if separated.shape[1] > separated.shape[2] and separated.shape[2] <= 4:
                            # å¯èƒ½æ˜¯ [B, T, S]ï¼Œéœ€è¦è½‰ç½®
                            separated = separated.transpose(1, 2)
                            logger.debug(f"Tiger è¼¸å‡ºè½‰ç½®å¾Œå½¢ç‹€: {separated.shape}")
                else:
                    # SepFormer ä½¿ç”¨ separate_batch æ–¹æ³•
                    # ç¢ºä¿è¼¸å…¥åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                    audio_tensor = audio_tensor.to(self.device)
                    
                    logger.debug(f"SepFormer è¼¸å…¥å½¢ç‹€: {audio_tensor.shape}, è¨­å‚™: {audio_tensor.device}")
                    separated = current_model.separate_batch(audio_tensor)
                    logger.debug(f"SepFormer åŸå§‹è¼¸å‡ºå½¢ç‹€: {separated.shape}, è¨­å‚™: {separated.device}")
                

                # 6) åƒ…åšå›ºå®šæ¯”ä¾‹çš„ã€Œå³°å€¼æ­£è¦åŒ–ã€ä»¥çµ±ä¸€å°ºåº¦ï¼ˆç¶­æŒåŸ _normalize_estimatesï¼‰
                #    ä¸åšä»»ä½•éŸ³è³ªå¢å¼·/æ¿¾æ³¢/æŠ•å½±å›æ··éŸ³ç­‰å¾Œè™•ç†
                separated, layout, spk_axis, time_axis = self._normalize_estimates(separated)
                logger.debug(f"æ­£è¦åŒ–å¾Œ - å½¢ç‹€: {separated.shape}, layout: {layout}, spk_axis: {spk_axis}, time_axis: {time_axis}")

                # 7) ä¾ layout å–å‡ºå€™é¸ï¼Œä¿ç•™å–®äººæƒ…å¢ƒçš„é¸è·¯é‚è¼¯ï¼ˆä½†ä»è¼¸å‡ºåŸå§‹åˆ†é›¢çµæœï¼‰
                raw_for_select = separated
                enhanced_separated = separated  # â† ä¸å†å‘¼å« enhance_separationï¼ˆå·²ç§»é™¤ï¼‰

                if layout == "BST":  # [B, S, T]
                    model_output_speakers = enhanced_separated.shape[spk_axis]
                    def _get_cand(idx):  return raw_for_select[0, idx, :].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, idx, :].detach().cpu()
                elif layout == "BTS":  # [B, T, S]
                    model_output_speakers = enhanced_separated.shape[2]
                    def _get_cand(idx):  return raw_for_select[0, :, idx].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, :, idx].detach().cpu()
                else:  # "BT"
                    model_output_speakers = 1
                    def _get_cand(idx):  return raw_for_select[0, :].detach().cpu()
                    def _get_final(idx): return enhanced_separated[0, :].detach().cpu()

                # å–®äººæƒ…å¢ƒçš„æœ€ä½³è·¯å¾‘é¸æ“‡ï¼ˆä¸æ¶‰åŠä»»ä½•éŸ³è¨Šè™•ç†ï¼Œåªæ˜¯é¸å“ªä¸€è·¯ï¼‰
                best_idx = 0
                if detected_speakers == 1 and model_output_speakers >= 2:
                    candidates = [_get_cand(j) for j in range(model_output_speakers)]
                    try:
                        # ç¢ºä¿å‚³çµ¦é¸è·¯å™¨çš„ audio_tensor åœ¨ CPU ä¸Š
                        mix_cpu = audio_tensor[0].detach().cpu() if audio_tensor.device != torch.device('cpu') else audio_tensor[0].detach()
                        best_idx, _, _ = self.single_selector.select(candidates, mix_cpu, return_stats=True)
                        
                        # é‡å»ºæ™‚ç¢ºä¿åœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
                        selected_audio = _get_final(best_idx).to(self.device)  # å…ˆç§»åˆ°è¨­å‚™
                        enhanced_separated = selected_audio.unsqueeze(0).unsqueeze(-1)  # [1, T, 1]
                        model_output_speakers = 1
                        layout = "BTS"
                        logger.debug(f"å–®äººé¸è·¯æˆåŠŸï¼Œé¸æ“‡é€šé“ {best_idx}, è¼¸å‡ºè¨­å‚™: {enhanced_separated.device}")
                    except Exception as e:
                        logger.exception(f"å–®äººé¸è·¯å¤±æ•—: {e}ï¼Œæ”¹ç”¨ speaker1 ä½œç‚ºä¿å®ˆè¼¸å‡º")
                        fallback_audio = _get_final(0).to(self.device)  # å…ˆç§»åˆ°è¨­å‚™
                        enhanced_separated = fallback_audio.unsqueeze(0).unsqueeze(-1)
                        model_output_speakers = 1
                        layout = "BTS"

                # 8) çµ±ä¸€æˆ [S, T] - æ ¹æ“š layout æ±ºå®šæ˜¯å¦è½‰ç½®
                # ç¢ºä¿ enhanced_separated åœ¨ CPU ä¸Šé€²è¡Œå¾ŒçºŒè™•ç†
                enhanced_separated = enhanced_separated.to('cpu')
                
                if enhanced_separated.ndim == 3:
                    if layout == "BST":  # [B, S, T] - Tiger é›™äººåŸå§‹è¼¸å‡º
                        est_ST = enhanced_separated[0].detach()
                    else:  # layout == "BTS" - å–®äººé‡æ§‹æˆ–å…¶ä»–
                        est_ST = enhanced_separated[0].transpose(0, 1).detach()
                else:
                    est_ST = enhanced_separated.detach().unsqueeze(0)

                if detected_speakers == 1 and est_ST.shape[0] >= 2:
                    # åªä¿ç•™è¢«é¸ä¸­çš„é‚£ä¸€è·¯
                    try:
                        est_ST = est_ST[best_idx:best_idx+1, :]
                    except Exception:
                        est_ST = est_ST[0:1, :]

                # 9) å„²å­˜åŸå§‹åˆ†é›¢çµæœï¼ˆä¸åš fadeã€ditherã€å“è³ªè©•åˆ†ã€pretty copyï¼‰
                S, T = est_ST.shape
                effective_speakers = min(int(detected_speakers), int(S), int(model_config["num_speakers"]))
                logger.debug(
                    f"åˆ†é›¢åƒæ•¸ - åµæ¸¬: {detected_speakers}, est_STé€šé“: {S}, "
                    f"æ¨¡å‹æ”¯æ´: {model_config['num_speakers']}, æœ‰æ•ˆ: {effective_speakers}"
                )

                saved_count = 0
                start_time = current_t0
                timestamp = datetime.now().strftime('%Y%m%d-%H_%M_%S')

                for i in range(effective_speakers):
                    try:
                        speaker_audio = est_ST[i].contiguous()  # 1D [T]
                        logger.debug(f"èªè€… {i+1} åŸå§‹éŸ³è¨Š - å½¢ç‹€: {speaker_audio.shape}, æ¨£æœ¬æ•¸: {speaker_audio.shape[0]}, æŒçºŒæ™‚é–“: {speaker_audio.shape[0]/TARGET_RATE:.3f}ç§’")
                        
                        # æª¢æŸ¥éŸ³è¨Šé•·åº¦æ˜¯å¦è¶³å¤ 
                        min_samples = int(0.5 * TARGET_RATE)  # è‡³å°‘ 0.5 ç§’
                        if speaker_audio.shape[0] < min_samples:
                            logger.warning(f"èªè€… {i+1} éŸ³è¨Šå¤ªçŸ­ ({speaker_audio.shape[0]} æ¨£æœ¬, {speaker_audio.shape[0]/TARGET_RATE:.3f}ç§’)ï¼Œè·³é")
                            continue
                        
                        # if self.enable_post_denoiser and self.denoiser is not None:
                        #     # ä½¿ç”¨è‡ªé©æ‡‰é™å™ªå™¨é€²è¡Œé™å™ª
                        #     try:
                        #         denoised_audio = self.denoiser.denoise(
                        #             audio=speaker_audio,  # 1D å¼µé‡
                        #             sample_rate=TARGET_RATE
                        #         )
                        #         logger.debug(f"èªè€… {i+1} é™å™ªå¾Œ - å½¢ç‹€: {denoised_audio.shape}")
                        #     except Exception as denoise_error:
                        #         logger.warning(f"èªè€… {i+1} é™å™ªå¤±æ•—: {denoise_error}ï¼Œä½¿ç”¨åŸå§‹éŸ³è¨Š")
                        #         denoised_audio = speaker_audio
                        # else:
                        #     denoised_audio = speaker_audio
                        denoised_audio = speaker_audio  # â† ä¸å†é€²è¡Œé™å™ªè™•ç†ï¼ˆå·²ç§»é™¤ï¼‰

                        final_tensor = denoised_audio.unsqueeze(0).cpu()  # [1, T]
                        logger.debug(f"èªè€… {i+1} æœ€çµ‚å¼µé‡ - å½¢ç‹€: {final_tensor.shape}, æŒçºŒæ™‚é–“: {final_tensor.shape[1]/TARGET_RATE:.3f}ç§’")

                        # å³°å€¼æ­£è¦åŒ–ä¸¦é™åˆ¶åœ¨ -1.0 åˆ° 1.0 ä¹‹é–“
                        final_tensor = torch.clamp(final_tensor * 0.98, -1.0, 1.0)
                        
                        output_file = os.path.join(
                            output_dir,
                            f"speaker{i+1}.wav"
                            # è‹¥è¦ä¿ç•™å‹•æ…‹æª”åå¯æ”¹ç‚ºï¼šf"speaker{i+1}_{timestamp}_{segment_index}.wav"
                        )
                        print("abs max before save:", float(torch.max(torch.abs(final_tensor))))
                        torchaudio.save(output_file, final_tensor, TARGET_RATE ,bits_per_sample=16)

                        absolute_timestamp = absolute_start_time.timestamp() + start_time
                        results.append((output_file, start_time, start_time + seg_duration, absolute_timestamp))
                        self.output_files.append(output_file)
                        saved_count += 1
                    except Exception as e:
                        logger.warning(f"å„²å­˜èªè€… {i+1} å¤±æ•—: {e}")

                if saved_count > 0:
                    logger.info(f"ç‰‡æ®µ {segment_index} å®Œæˆï¼Œå„²å­˜ {saved_count}/{effective_speakers} å€‹æª”æ¡ˆ (ä½¿ç”¨ {current_model_type.value})")

            # 10) æ›´æ–°æ™‚é–“ç´¯è¨ˆ
            current_t0 += seg_duration
            self._current_t0 = current_t0

            if not results:
                raise RuntimeError("Speaker separation produced no valid tracks")

            return results

        except Exception as e:
            logger.error(f"è™•ç†ç‰‡æ®µ {segment_index} å¤±æ•—: {e}")
            self.processing_stats['errors'] += 1
        finally:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def stop_recording(self):
        """åœæ­¢éŒ„éŸ³"""
        self.is_recording = False
        logger.info("æº–å‚™åœæ­¢éŒ„éŸ³...")

    def get_output_files(self):
        """ç²å–æ‰€æœ‰åˆ†é›¢å¾Œçš„éŸ³æª”è·¯å¾‘"""
        return self.output_files

# æ·»åŠ å…¨åŸŸå‡½å¼ä¾†ç®¡ç†å¿«å–
def get_cached_separator(model_type: SeparationModel = DEFAULT_MODEL, enable_dynamic_model: bool = True, **kwargs) -> AudioSeparator:
    """
    å–å¾—å¿«å–çš„ AudioSeparator å¯¦ä¾‹ï¼Œé¿å…é‡è¤‡åˆå§‹åŒ–
    
    Args:
        model_type: æ¨¡å‹é¡å‹
        enable_dynamic_model: æ˜¯å¦å•Ÿç”¨å‹•æ…‹æ¨¡å‹
        **kwargs: å…¶ä»–åƒæ•¸
    
    Returns:
        AudioSeparator å¯¦ä¾‹
    """
    global _GLOBAL_SEPARATOR_CACHE
    
    # å»ºç«‹å¿«å–éµ
    cache_key = f"{model_type.value}_{enable_dynamic_model}_{hash(tuple(sorted(kwargs.items())))}"
    
    # æª¢æŸ¥å¿«å–
    if cache_key in _GLOBAL_SEPARATOR_CACHE:
        logger.info(f"ä½¿ç”¨å¿«å–çš„ AudioSeparator: {cache_key}")
        return _GLOBAL_SEPARATOR_CACHE[cache_key]
    
    # å»ºç«‹æ–°å¯¦ä¾‹ä¸¦å¿«å–
    logger.info(f"å»ºç«‹æ–°çš„ AudioSeparator: {cache_key}")
    separator = AudioSeparator(
        model_type=model_type, 
        enable_dynamic_model=enable_dynamic_model, 
        **kwargs
    )
    _GLOBAL_SEPARATOR_CACHE[cache_key] = separator
    
    return separator

def clear_separator_cache():
    """æ¸…ç†æ‰€æœ‰å¿«å–çš„åˆ†é›¢å™¨å¯¦ä¾‹"""
    global _GLOBAL_SEPARATOR_CACHE, _GLOBAL_SPEAKER_PIPELINE_CACHE
    
    # æ¸…ç†åˆ†é›¢å™¨å¿«å–
    for separator in _GLOBAL_SEPARATOR_CACHE.values():
        try:
            if hasattr(separator, 'model_manager') and separator.model_manager:
                separator.model_manager.cleanup()
        except Exception as e:
            logger.warning(f"æ¸…ç†åˆ†é›¢å™¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    _GLOBAL_SEPARATOR_CACHE.clear()
    
    # æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–
    if _GLOBAL_SPEAKER_PIPELINE_CACHE is not None:
        try:
            del _GLOBAL_SPEAKER_PIPELINE_CACHE
            _GLOBAL_SPEAKER_PIPELINE_CACHE = None
            logger.info("å·²æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–")
        except Exception as e:
            logger.warning(f"æ¸…ç†èªè€…è¨ˆæ•¸ç®¡ç·šå¿«å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
    # æ¸…ç†é™å™ªå™¨å¿«å–
    try:
        from .post_adaptive_denoiser import clear_denoiser_cache
        clear_denoiser_cache()
        logger.info("å·²æ¸…ç†é™å™ªå™¨å¿«å–")
    except Exception as e:
        logger.warning(f"æ¸…ç†é™å™ªå™¨å¿«å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def check_weaviate_connection() -> bool:
    """
    æª¢æŸ¥ Weaviate è³‡æ–™åº«é€£ç·šç‹€æ…‹ã€‚

    Returns:
        bool: è‹¥é€£ç·šæˆåŠŸå›å‚³ Trueï¼Œå¦å‰‡å›å‚³ Falseã€‚
    """
    try:
        import weaviate  # type: ignore
        client = weaviate.connect_to_local()
        # æª¢æŸ¥æ˜¯å¦èƒ½å­˜å–å¿…è¦é›†åˆ
        if not client.is_live():
            logger.error("Weaviate æœå‹™æœªå•Ÿå‹•æˆ–ç„¡æ³•å­˜å–ã€‚")
            return False
        if not (client.collections.exists("Speaker") and client.collections.exists("VoicePrint")):
            logger.error("Weaviate ç¼ºå°‘å¿…è¦é›†åˆ (Speaker æˆ– VoicePrint)ã€‚è«‹å…ˆåŸ·è¡Œ create_collections.pyã€‚")
            return False
        return True
    except Exception as e:
        logger.error(f"Weaviate é€£ç·šå¤±æ•—ï¼š{e}")
        return False
    