"""
===============================================================================
éŸ³è¨Šå“è³ªè©•ä¼°å·¥å…· (Audio Quality Assessment Tool)
===============================================================================

ç‰ˆæœ¬ï¼šv1.0.0
æœ€å¾Œæ›´æ–°ï¼š2025-10-25

åŠŸèƒ½ï¼š
-----
1. å°æ¯”æ–°èˆŠæ–¹æ³•åˆ†é›¢å‡ºçš„éŸ³è¨Šå“è³ª
2. æä¾›å¤šç¶­åº¦çš„å®¢è§€è©•ä¼°æŒ‡æ¨™
3. ç”Ÿæˆè©³ç´°çš„å°æ¯”å ±å‘Š
4. æ”¯æ´æ‰¹æ¬¡è©•ä¼°å¤šå€‹æª”æ¡ˆ

è©•ä¼°æŒ‡æ¨™ï¼š
---------
ğŸ“Š åŸºç¤æŒ‡æ¨™ï¼š
 â€¢ å³°å€¼ (Peak)ï¼šéŸ³è¨Šæœ€å¤§æŒ¯å¹…
 â€¢ RMSï¼šå‡æ–¹æ ¹èƒ½é‡
 â€¢ å‹•æ…‹ç¯„åœ (Dynamic Range)ï¼šæœ€å¤§èˆ‡æœ€å°èƒ½é‡çš„æ¯”å€¼

ğŸ“ˆ é »è­œæŒ‡æ¨™ï¼š
 â€¢ é »è­œå¹³å¦åº¦ (Spectral Flatness)ï¼šé »è­œå¹³æ»‘ç¨‹åº¦
 â€¢ é »è­œè³ªå¿ƒ (Spectral Centroid)ï¼šé »ç‡é‡å¿ƒä½ç½®
 â€¢ é«˜é »èƒ½é‡æ¯” (High Frequency Ratio)ï¼šé«˜é »ä¿ç•™ç¨‹åº¦

ğŸ¯ èªéŸ³å“è³ªæŒ‡æ¨™ï¼š
 â€¢ æ¸…æ™°åº¦åˆ†æ•¸ (Clarity Score)ï¼šèªéŸ³é »æ®µèƒ½é‡
 â€¢ SNR ä¼°è¨ˆï¼šä¿¡å™ªæ¯”
 â€¢ é›¶äº¤å‰ç‡ (Zero Crossing Rate)ï¼šéŸ³è¨Šæ¸…æ™°åº¦

ğŸ”Š æ„ŸçŸ¥æŒ‡æ¨™ï¼š
 â€¢ éŸ¿åº¦ (Loudness)ï¼šæ„ŸçŸ¥éŸ³é‡
 â€¢ å°–éŠ³åº¦ (Sharpness)ï¼šé«˜é »æ„ŸçŸ¥

ä½¿ç”¨æ–¹å¼ï¼š
---------
python audio_quality_assessment.py \
    --old_dir ./old_outputs \
    --new_dir ./new_outputs \
    --output_report ./quality_report.html

æˆ–ä½¿ç”¨ Python APIï¼š
from audio_quality_assessment import AudioQualityAssessment

evaluator = AudioQualityAssessment()
results = evaluator.compare_directories(
    old_dir="./old_outputs",
    new_dir="./new_outputs"
)
evaluator.generate_html_report(results, "report.html")

===============================================================================
"""

import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime

# è¨­å®šä¸­æ–‡å­—å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
try:
    plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'Arial Unicode MS', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass


@dataclass
class AudioMetrics:
    """éŸ³è¨Šè©•ä¼°æŒ‡æ¨™"""
    # åŸºç¤æŒ‡æ¨™
    peak: float
    rms: float
    dynamic_range: float
    
    # é »è­œæŒ‡æ¨™
    spectral_flatness: float
    spectral_centroid: float
    high_freq_ratio: float
    
    # èªéŸ³å“è³ª
    clarity_score: float
    snr_estimate: float
    zero_crossing_rate: float
    
    # æ„ŸçŸ¥æŒ‡æ¨™
    loudness: float
    sharpness: float
    
    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            'basic': {
                'peak': float(self.peak),
                'rms': float(self.rms),
                'dynamic_range': float(self.dynamic_range)
            },
            'spectral': {
                'spectral_flatness': float(self.spectral_flatness),
                'spectral_centroid': float(self.spectral_centroid),
                'high_freq_ratio': float(self.high_freq_ratio)
            },
            'speech_quality': {
                'clarity_score': float(self.clarity_score),
                'snr_estimate': float(self.snr_estimate),
                'zero_crossing_rate': float(self.zero_crossing_rate)
            },
            'perceptual': {
                'loudness': float(self.loudness),
                'sharpness': float(self.sharpness)
            }
        }


class AudioQualityAssessment:
    """
    éŸ³è¨Šå“è³ªè©•ä¼°å·¥å…·
    
    ç”¨æ–¼å°æ¯”æ–°èˆŠæ–¹æ³•åˆ†é›¢å‡ºçš„éŸ³è¨Šå“è³ªå·®ç•°
    """
    
    def __init__(self, sample_rate: int = 16000, verbose: bool = True):
        """
        åˆå§‹åŒ–è©•ä¼°å·¥å…·
        
        Args:
            sample_rate: ç›®æ¨™æ¡æ¨£ç‡
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è³‡è¨Š
        """
        self.sr = sample_rate
        self.verbose = verbose
    
    def load_audio(self, filepath: str) -> Tuple[torch.Tensor, int]:
        """
        è¼‰å…¥éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            filepath: éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
        
        Returns:
            (audio, sample_rate): éŸ³è¨Šå¼µé‡å’Œæ¡æ¨£ç‡
        """
        audio, sr = torchaudio.load(filepath)
        
        # è½‰ç‚ºå–®è²é“
        if audio.shape[0] > 1:
            audio = audio.mean(dim=0, keepdim=True)
        
        # é‡æ¡æ¨£
        if sr != self.sr:
            resampler = torchaudio.transforms.Resample(sr, self.sr)
            audio = resampler(audio)
        
        return audio.squeeze(0), self.sr
    
    def compute_basic_metrics(self, audio: torch.Tensor) -> dict:
        """
        è¨ˆç®—åŸºç¤éŸ³è¨ŠæŒ‡æ¨™
        
        Args:
            audio: 1D éŸ³è¨Šå¼µé‡
        
        Returns:
            metrics: åŸºç¤æŒ‡æ¨™å­—å…¸
        """
        # å³°å€¼
        peak = audio.abs().max().item()
        
        # RMS
        rms = torch.sqrt(audio.pow(2).mean()).item()
        
        # å‹•æ…‹ç¯„åœ
        frame_length = int(0.025 * self.sr)
        hop_length = int(0.010 * self.sr)
        
        energies = []
        for i in range(0, len(audio) - frame_length, hop_length):
            frame = audio[i:i+frame_length]
            energy = frame.pow(2).mean().item()
            if energy > 1e-10:
                energies.append(energy)
        
        if len(energies) > 0:
            dynamic_range = 10 * np.log10(max(energies) / (min(energies) + 1e-10))
        else:
            dynamic_range = 0.0
        
        return {
            'peak': peak,
            'rms': rms,
            'dynamic_range': dynamic_range
        }
    
    def compute_spectral_metrics(self, audio: torch.Tensor) -> dict:
        """
        è¨ˆç®—é »è­œæŒ‡æ¨™
        
        Args:
            audio: 1D éŸ³è¨Šå¼µé‡
        
        Returns:
            metrics: é »è­œæŒ‡æ¨™å­—å…¸
        """
        # STFT
        stft = torch.stft(
            audio,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        
        magnitude = stft.abs()
        power = magnitude.pow(2)
        
        # é »è­œå¹³å¦åº¦
        geometric_mean = torch.exp(torch.log(magnitude + 1e-10).mean())
        arithmetic_mean = magnitude.mean()
        spectral_flatness = (geometric_mean / (arithmetic_mean + 1e-10)).item()
        
        # é »è­œè³ªå¿ƒ
        freqs = torch.fft.rfftfreq(512, 1/self.sr, device=audio.device)
        weighted_sum = (freqs.unsqueeze(1) * power).sum()
        total_power = power.sum()
        spectral_centroid = (weighted_sum / (total_power + 1e-10)).item()
        
        # é«˜é »èƒ½é‡æ¯” (2kHz ä»¥ä¸Š)
        high_freq_mask = freqs >= 2000
        high_freq_power = power[high_freq_mask].sum()
        total_power = power.sum()
        high_freq_ratio = (high_freq_power / (total_power + 1e-10)).item()
        
        return {
            'spectral_flatness': spectral_flatness,
            'spectral_centroid': spectral_centroid,
            'high_freq_ratio': high_freq_ratio
        }
    
    def compute_speech_quality_metrics(self, audio: torch.Tensor) -> dict:
        """
        è¨ˆç®—èªéŸ³å“è³ªæŒ‡æ¨™
        
        Args:
            audio: 1D éŸ³è¨Šå¼µé‡
        
        Returns:
            metrics: èªéŸ³å“è³ªæŒ‡æ¨™å­—å…¸
        """
        # STFT
        stft = torch.stft(
            audio,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        
        magnitude = stft.abs()
        power = magnitude.pow(2).mean(dim=-1)
        
        # æ¸…æ™°åº¦åˆ†æ•¸ï¼ˆèªéŸ³é »æ®µ 300-3400 Hz çš„èƒ½é‡ä½”æ¯”ï¼‰
        freqs = torch.fft.rfftfreq(512, 1/self.sr, device=audio.device)
        speech_mask = (freqs >= 300) & (freqs <= 3400)
        speech_energy = power[speech_mask].sum()
        total_energy = power.sum()
        clarity_score = (speech_energy / (total_energy + 1e-10)).item()
        
        # SNR ä¼°è¨ˆ
        frame_length = int(0.025 * self.sr)
        hop_length = int(0.010 * self.sr)
        
        energies = []
        for i in range(0, len(audio) - frame_length, hop_length):
            frame = audio[i:i+frame_length]
            energy = frame.pow(2).mean().item()
            energies.append(energy)
        
        if len(energies) > 0:
            energies = np.array(energies)
            energy_threshold = np.percentile(energies, 40)
            speech_frames = energies > energy_threshold
            noise_frames = ~speech_frames
            
            if speech_frames.sum() > 0 and noise_frames.sum() > 0:
                speech_energy = energies[speech_frames].mean()
                noise_energy = energies[noise_frames].mean()
                snr_estimate = 10 * np.log10(speech_energy / (noise_energy + 1e-10))
            else:
                snr_estimate = 20.0
        else:
            snr_estimate = 20.0
        
        # é›¶äº¤å‰ç‡
        zero_crossings = torch.sum(torch.diff(torch.sign(audio)) != 0).item()
        zero_crossing_rate = zero_crossings / len(audio)
        
        return {
            'clarity_score': clarity_score,
            'snr_estimate': snr_estimate,
            'zero_crossing_rate': zero_crossing_rate
        }
    
    def compute_perceptual_metrics(self, audio: torch.Tensor) -> dict:
        """
        è¨ˆç®—æ„ŸçŸ¥æŒ‡æ¨™
        
        Args:
            audio: 1D éŸ³è¨Šå¼µé‡
        
        Returns:
            metrics: æ„ŸçŸ¥æŒ‡æ¨™å­—å…¸
        """
        # STFT
        stft = torch.stft(
            audio,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        
        magnitude = stft.abs()
        power = magnitude.pow(2).mean(dim=-1)
        
        # éŸ¿åº¦ï¼ˆè¿‘ä¼¼ï¼‰
        loudness = 10 * torch.log10(power.mean() + 1e-10).item()
        
        # å°–éŠ³åº¦ï¼ˆé«˜é »æ¬Šé‡ï¼‰
        freqs = torch.fft.rfftfreq(512, 1/self.sr, device=audio.device)
        weights = torch.where(freqs > 1000, (freqs / 1000).pow(2), torch.ones_like(freqs))
        weighted_power = (power * weights).sum()
        sharpness = (weighted_power / (power.sum() + 1e-10)).item()
        
        return {
            'loudness': loudness,
            'sharpness': sharpness
        }
    
    def evaluate_audio(self, filepath: str) -> AudioMetrics:
        """
        å®Œæ•´è©•ä¼°ä¸€å€‹éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            filepath: éŸ³è¨Šæª”æ¡ˆè·¯å¾‘
        
        Returns:
            metrics: å®Œæ•´çš„éŸ³è¨ŠæŒ‡æ¨™
        """
        if self.verbose:
            print(f"è©•ä¼°: {os.path.basename(filepath)}")
        
        # è¼‰å…¥éŸ³è¨Š
        audio, _ = self.load_audio(filepath)
        
        # è¨ˆç®—å„é¡æŒ‡æ¨™
        basic = self.compute_basic_metrics(audio)
        spectral = self.compute_spectral_metrics(audio)
        speech = self.compute_speech_quality_metrics(audio)
        perceptual = self.compute_perceptual_metrics(audio)
        
        return AudioMetrics(
            peak=basic['peak'],
            rms=basic['rms'],
            dynamic_range=basic['dynamic_range'],
            spectral_flatness=spectral['spectral_flatness'],
            spectral_centroid=spectral['spectral_centroid'],
            high_freq_ratio=spectral['high_freq_ratio'],
            clarity_score=speech['clarity_score'],
            snr_estimate=speech['snr_estimate'],
            zero_crossing_rate=speech['zero_crossing_rate'],
            loudness=perceptual['loudness'],
            sharpness=perceptual['sharpness']
        )
    
    def compare_files(
        self,
        old_file: str,
        new_file: str
    ) -> Dict[str, any]:
        """
        å°æ¯”å…©å€‹éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            old_file: èˆŠæ–¹æ³•çš„éŸ³è¨Šæª”æ¡ˆ
            new_file: æ–°æ–¹æ³•çš„éŸ³è¨Šæª”æ¡ˆ
        
        Returns:
            comparison: å°æ¯”çµæœ
        """
        old_metrics = self.evaluate_audio(old_file)
        new_metrics = self.evaluate_audio(new_file)
        
        # è¨ˆç®—æ”¹å–„ç™¾åˆ†æ¯”
        improvements = {}
        
        # åŸºç¤æŒ‡æ¨™ï¼ˆè¶Šé«˜è¶Šå¥½ï¼špeak æ¥è¿‘ 1.0, rms é©ä¸­, dynamic_range é«˜ï¼‰
        improvements['peak_improvement'] = (new_metrics.peak - old_metrics.peak) / old_metrics.peak * 100
        improvements['rms_improvement'] = (new_metrics.rms - old_metrics.rms) / old_metrics.rms * 100
        improvements['dynamic_range_improvement'] = (new_metrics.dynamic_range - old_metrics.dynamic_range) / (old_metrics.dynamic_range + 1e-10) * 100
        
        # é »è­œæŒ‡æ¨™ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        improvements['spectral_flatness_improvement'] = (new_metrics.spectral_flatness - old_metrics.spectral_flatness) / old_metrics.spectral_flatness * 100
        improvements['high_freq_ratio_improvement'] = (new_metrics.high_freq_ratio - old_metrics.high_freq_ratio) / old_metrics.high_freq_ratio * 100
        
        # èªéŸ³å“è³ªï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        improvements['clarity_improvement'] = (new_metrics.clarity_score - old_metrics.clarity_score) / old_metrics.clarity_score * 100
        improvements['snr_improvement'] = (new_metrics.snr_estimate - old_metrics.snr_estimate) / (old_metrics.snr_estimate + 1e-10) * 100
        
        # æ•´é«”å“è³ªåˆ†æ•¸ï¼ˆ0-100ï¼‰
        old_score = self._calculate_overall_score(old_metrics)
        new_score = self._calculate_overall_score(new_metrics)
        improvements['overall_improvement'] = new_score - old_score
        
        return {
            'old_metrics': old_metrics,
            'new_metrics': new_metrics,
            'improvements': improvements,
            'old_score': old_score,
            'new_score': new_score
        }
    
    def _calculate_overall_score(self, metrics: AudioMetrics) -> float:
        """
        è¨ˆç®—æ•´é«”å“è³ªåˆ†æ•¸ (0-100)
        
        æ¬Šé‡åˆ†é…ï¼š
        - å³°å€¼ (10%)ï¼šæ¥è¿‘ 0.98 æœ€å¥½
        - RMS (10%)ï¼š0.1-0.2 æœ€å¥½
        - å‹•æ…‹ç¯„åœ (15%)ï¼šè¶Šé«˜è¶Šå¥½
        - æ¸…æ™°åº¦ (20%)ï¼šè¶Šé«˜è¶Šå¥½
        - SNR (25%)ï¼šè¶Šé«˜è¶Šå¥½
        - é«˜é »ä¿ç•™ (10%)ï¼š0.15-0.25 æœ€å¥½
        - é »è­œå¹³å¦åº¦ (10%)ï¼š0.2-0.4 æœ€å¥½
        """
        score = 0.0
        
        # å³°å€¼åˆ†æ•¸ï¼ˆæ¥è¿‘ 0.98 æœ€å¥½ï¼‰
        peak_score = max(0, 100 - abs(metrics.peak - 0.98) * 500)
        score += peak_score * 0.10
        
        # RMS åˆ†æ•¸ï¼ˆ0.1-0.2 æœ€å¥½ï¼‰
        if 0.1 <= metrics.rms <= 0.2:
            rms_score = 100
        else:
            rms_score = max(0, 100 - abs(metrics.rms - 0.15) * 300)
        score += rms_score * 0.10
        
        # å‹•æ…‹ç¯„åœåˆ†æ•¸ï¼ˆ20-60 dB æ­£å¸¸ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
        dr_score = min(100, (metrics.dynamic_range / 60) * 100)
        score += dr_score * 0.15
        
        # æ¸…æ™°åº¦åˆ†æ•¸ï¼ˆ0.6-0.8 æœ€å¥½ï¼‰
        if 0.6 <= metrics.clarity_score <= 0.8:
            clarity_score = 100
        else:
            clarity_score = max(0, 100 - abs(metrics.clarity_score - 0.7) * 200)
        score += clarity_score * 0.20
        
        # SNR åˆ†æ•¸ï¼ˆ15-30 dB æ­£å¸¸ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
        snr_score = min(100, (metrics.snr_estimate / 30) * 100)
        score += snr_score * 0.25
        
        # é«˜é »ä¿ç•™åˆ†æ•¸ï¼ˆ0.15-0.25 æœ€å¥½ï¼‰
        if 0.15 <= metrics.high_freq_ratio <= 0.25:
            hf_score = 100
        else:
            hf_score = max(0, 100 - abs(metrics.high_freq_ratio - 0.20) * 300)
        score += hf_score * 0.10
        
        # é »è­œå¹³å¦åº¦åˆ†æ•¸ï¼ˆ0.2-0.4 æœ€å¥½ï¼‰
        if 0.2 <= metrics.spectral_flatness <= 0.4:
            sf_score = 100
        else:
            sf_score = max(0, 100 - abs(metrics.spectral_flatness - 0.3) * 200)
        score += sf_score * 0.10
        
        return score
    
    def compare_directories(
        self,
        old_dir: str,
        new_dir: str,
        pattern: str = "*.wav"
    ) -> Dict[str, any]:
        """
        å°æ¯”å…©å€‹ç›®éŒ„ä¸­çš„æ‰€æœ‰éŸ³è¨Šæª”æ¡ˆ
        
        Args:
            old_dir: èˆŠæ–¹æ³•è¼¸å‡ºç›®éŒ„
            new_dir: æ–°æ–¹æ³•è¼¸å‡ºç›®éŒ„
            pattern: æª”æ¡ˆåŒ¹é…æ¨¡å¼
        
        Returns:
            results: å®Œæ•´å°æ¯”çµæœ
        """
        old_path = Path(old_dir)
        new_path = Path(new_dir)
        
        # æ‰¾åˆ°æ‰€æœ‰éŸ³è¨Šæª”æ¡ˆ
        old_files = sorted(old_path.glob(pattern))
        new_files = sorted(new_path.glob(pattern))
        
        if self.verbose:
            print(f"\næ‰¾åˆ° {len(old_files)} å€‹èˆŠæª”æ¡ˆï¼Œ{len(new_files)} å€‹æ–°æª”æ¡ˆ")
        
        # é…å°æª”æ¡ˆï¼ˆåŸºæ–¼æª”åï¼‰
        comparisons = []
        for old_file in old_files:
            new_file = new_path / old_file.name
            if new_file.exists():
                if self.verbose:
                    print(f"\nå°æ¯”: {old_file.name}")
                
                comparison = self.compare_files(str(old_file), str(new_file))
                comparison['filename'] = old_file.name
                comparisons.append(comparison)
            else:
                if self.verbose:
                    print(f"âš ï¸ æ‰¾ä¸åˆ°å°æ‡‰çš„æ–°æª”æ¡ˆ: {old_file.name}")
        
        # è¨ˆç®—å¹³å‡æ”¹å–„
        if comparisons:
            avg_improvements = {}
            for key in comparisons[0]['improvements'].keys():
                values = [c['improvements'][key] for c in comparisons]
                avg_improvements[key] = np.mean(values)
            
            avg_old_score = np.mean([c['old_score'] for c in comparisons])
            avg_new_score = np.mean([c['new_score'] for c in comparisons])
        else:
            avg_improvements = {}
            avg_old_score = 0
            avg_new_score = 0
        
        return {
            'comparisons': comparisons,
            'avg_improvements': avg_improvements,
            'avg_old_score': avg_old_score,
            'avg_new_score': avg_new_score,
            'timestamp': datetime.now().isoformat()
        }
    
    def generate_report(
        self,
        results: Dict[str, any],
        output_file: str = "quality_report.txt"
    ):
        """
        ç”Ÿæˆæ–‡å­—æ ¼å¼çš„è©•ä¼°å ±å‘Š
        
        Args:
            results: compare_directories çš„çµæœ
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("éŸ³è¨Šå“è³ªè©•ä¼°å ±å‘Š\n")
            f.write("="*80 + "\n\n")
            f.write(f"è©•ä¼°æ™‚é–“: {results['timestamp']}\n")
            f.write(f"è©•ä¼°æª”æ¡ˆæ•¸: {len(results['comparisons'])}\n\n")
            
            # æ•´é«”æ”¹å–„æ‘˜è¦
            f.write("-"*80 + "\n")
            f.write("æ•´é«”æ”¹å–„æ‘˜è¦\n")
            f.write("-"*80 + "\n")
            f.write(f"èˆŠæ–¹æ³•å¹³å‡åˆ†æ•¸: {results['avg_old_score']:.2f}/100\n")
            f.write(f"æ–°æ–¹æ³•å¹³å‡åˆ†æ•¸: {results['avg_new_score']:.2f}/100\n")
            f.write(f"æ•´é«”æ”¹å–„: {results['avg_new_score'] - results['avg_old_score']:+.2f} åˆ†\n\n")
            
            # å„é …æŒ‡æ¨™å¹³å‡æ”¹å–„
            f.write("å„é …æŒ‡æ¨™å¹³å‡æ”¹å–„:\n")
            f.write(f"  å³°å€¼: {results['avg_improvements']['peak_improvement']:+.2f}%\n")
            f.write(f"  RMS: {results['avg_improvements']['rms_improvement']:+.2f}%\n")
            f.write(f"  å‹•æ…‹ç¯„åœ: {results['avg_improvements']['dynamic_range_improvement']:+.2f}%\n")
            f.write(f"  æ¸…æ™°åº¦: {results['avg_improvements']['clarity_improvement']:+.2f}%\n")
            f.write(f"  SNR: {results['avg_improvements']['snr_improvement']:+.2f}%\n")
            f.write(f"  é«˜é »ä¿ç•™: {results['avg_improvements']['high_freq_ratio_improvement']:+.2f}%\n\n")
            
            # å€‹åˆ¥æª”æ¡ˆè©³ç´°çµæœ
            f.write("-"*80 + "\n")
            f.write("å€‹åˆ¥æª”æ¡ˆè©³ç´°çµæœ\n")
            f.write("-"*80 + "\n\n")
            
            for comp in results['comparisons']:
                f.write(f"æª”æ¡ˆ: {comp['filename']}\n")
                f.write(f"  èˆŠæ–¹æ³•åˆ†æ•¸: {comp['old_score']:.2f}/100\n")
                f.write(f"  æ–°æ–¹æ³•åˆ†æ•¸: {comp['new_score']:.2f}/100\n")
                f.write(f"  æ”¹å–„: {comp['improvements']['overall_improvement']:+.2f} åˆ†\n")
                f.write("\n  è©³ç´°æŒ‡æ¨™:\n")
                
                old = comp['old_metrics']
                new = comp['new_metrics']
                
                f.write(f"    å³°å€¼: {old.peak:.4f} â†’ {new.peak:.4f} ({comp['improvements']['peak_improvement']:+.2f}%)\n")
                f.write(f"    RMS: {old.rms:.4f} â†’ {new.rms:.4f} ({comp['improvements']['rms_improvement']:+.2f}%)\n")
                f.write(f"    å‹•æ…‹ç¯„åœ: {old.dynamic_range:.2f} â†’ {new.dynamic_range:.2f} dB ({comp['improvements']['dynamic_range_improvement']:+.2f}%)\n")
                f.write(f"    æ¸…æ™°åº¦: {old.clarity_score:.4f} â†’ {new.clarity_score:.4f} ({comp['improvements']['clarity_improvement']:+.2f}%)\n")
                f.write(f"    SNR: {old.snr_estimate:.2f} â†’ {new.snr_estimate:.2f} dB ({comp['improvements']['snr_improvement']:+.2f}%)\n")
                f.write(f"    é«˜é »æ¯”: {old.high_freq_ratio:.4f} â†’ {new.high_freq_ratio:.4f} ({comp['improvements']['high_freq_ratio_improvement']:+.2f}%)\n")
                f.write("\n")
        
        if self.verbose:
            print(f"\nâœ“ å ±å‘Šå·²å„²å­˜è‡³: {output_file}")
    
    def generate_html_report(
        self,
        results: Dict[str, any],
        output_file: str = "quality_report.html"
    ):
        """
        ç”Ÿæˆ HTML æ ¼å¼çš„è©•ä¼°å ±å‘Šï¼ˆå«åœ–è¡¨ï¼‰
        
        Args:
            results: compare_directories çš„çµæœ
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        # æº–å‚™æ•¸æ“š
        filenames = [c['filename'] for c in results['comparisons']]
        old_scores = [c['old_score'] for c in results['comparisons']]
        new_scores = [c['new_score'] for c in results['comparisons']]
        
        # å‰µå»ºåœ–è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # åœ–1: æ•´é«”åˆ†æ•¸å°æ¯”
        ax = axes[0, 0]
        x = np.arange(len(filenames))
        width = 0.35
        ax.bar(x - width/2, old_scores, width, label='èˆŠæ–¹æ³•', alpha=0.8)
        ax.bar(x + width/2, new_scores, width, label='æ–°æ–¹æ³•', alpha=0.8)
        ax.set_xlabel('æª”æ¡ˆ')
        ax.set_ylabel('åˆ†æ•¸')
        ax.set_title('æ•´é«”å“è³ªåˆ†æ•¸å°æ¯”')
        ax.set_xticks(x)
        ax.set_xticklabels([f[:10] for f in filenames], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # åœ–2: æ”¹å–„ç™¾åˆ†æ¯”
        ax = axes[0, 1]
        improvements = [c['improvements']['overall_improvement'] for c in results['comparisons']]
        colors = ['green' if i > 0 else 'red' for i in improvements]
        ax.bar(range(len(filenames)), improvements, color=colors, alpha=0.7)
        ax.set_xlabel('æª”æ¡ˆ')
        ax.set_ylabel('æ”¹å–„åˆ†æ•¸')
        ax.set_title('å“è³ªæ”¹å–„ç¨‹åº¦')
        ax.set_xticks(range(len(filenames)))
        ax.set_xticklabels([f[:10] for f in filenames], rotation=45, ha='right')
        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax.grid(True, alpha=0.3)
        
        # åœ–3: å„é …æŒ‡æ¨™å¹³å‡æ”¹å–„
        ax = axes[1, 0]
        metrics = ['å³°å€¼', 'RMS', 'å‹•æ…‹ç¯„åœ', 'æ¸…æ™°åº¦', 'SNR', 'é«˜é »']
        improvements_values = [
            results['avg_improvements']['peak_improvement'],
            results['avg_improvements']['rms_improvement'],
            results['avg_improvements']['dynamic_range_improvement'],
            results['avg_improvements']['clarity_improvement'],
            results['avg_improvements']['snr_improvement'],
            results['avg_improvements']['high_freq_ratio_improvement']
        ]
        colors = ['green' if i > 0 else 'red' for i in improvements_values]
        ax.barh(metrics, improvements_values, color=colors, alpha=0.7)
        ax.set_xlabel('æ”¹å–„ (%)')
        ax.set_title('å„é …æŒ‡æ¨™å¹³å‡æ”¹å–„')
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
        ax.grid(True, alpha=0.3)
        
        # åœ–4: åˆ†æ•¸åˆ†å¸ƒ
        ax = axes[1, 1]
        ax.hist([old_scores, new_scores], bins=10, label=['èˆŠæ–¹æ³•', 'æ–°æ–¹æ³•'], alpha=0.7)
        ax.set_xlabel('åˆ†æ•¸')
        ax.set_ylabel('æª”æ¡ˆæ•¸')
        ax.set_title('åˆ†æ•¸åˆ†å¸ƒ')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨
        chart_file = output_file.replace('.html', '_chart.png')
        plt.savefig(chart_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        # ç”Ÿæˆ HTML
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>éŸ³è¨Šå“è³ªè©•ä¼°å ±å‘Š</title>
    <style>
        body {{
            font-family: 'Microsoft JhengHei', Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        h2 {{
            color: #34495e;
            margin-top: 30px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 5px;
        }}
        .summary {{
            background-color: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }}
        .summary-item {{
            display: flex;
            justify-content: space-between;
            padding: 10px 0;
            border-bottom: 1px solid #bdc3c7;
        }}
        .summary-item:last-child {{
            border-bottom: none;
        }}
        .score {{
            font-size: 24px;
            font-weight: bold;
        }}
        .score.old {{
            color: #e74c3c;
        }}
        .score.new {{
            color: #27ae60;
        }}
        .improvement {{
            color: #27ae60;
            font-weight: bold;
        }}
        .improvement.negative {{
            color: #e74c3c;
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        th, td {{
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ecf0f1;
        }}
        th {{
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }}
        tr:hover {{
            background-color: #f8f9fa;
        }}
        .chart {{
            text-align: center;
            margin: 30px 0;
        }}
        .chart img {{
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .metric-card {{
            display: inline-block;
            width: 30%;
            margin: 10px 1%;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 5px;
            vertical-align: top;
        }}
        .metric-name {{
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 5px;
        }}
        .metric-value {{
            font-size: 14px;
            color: #7f8c8d;
        }}
        .arrow {{
            font-size: 18px;
            margin: 0 5px;
        }}
        .footer {{
            margin-top: 30px;
            padding-top: 20px;
            border-top: 2px solid #ecf0f1;
            text-align: center;
            color: #7f8c8d;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸµ éŸ³è¨Šå“è³ªè©•ä¼°å ±å‘Š</h1>
        
        <div class="summary">
            <div class="summary-item">
                <span>è©•ä¼°æ™‚é–“:</span>
                <span>{results['timestamp']}</span>
            </div>
            <div class="summary-item">
                <span>è©•ä¼°æª”æ¡ˆæ•¸:</span>
                <span>{len(results['comparisons'])} å€‹</span>
            </div>
        </div>
        
        <h2>ğŸ“Š æ•´é«”æ”¹å–„æ‘˜è¦</h2>
        <div class="summary">
            <div class="summary-item">
                <span>èˆŠæ–¹æ³•å¹³å‡åˆ†æ•¸:</span>
                <span class="score old">{results['avg_old_score']:.2f}/100</span>
            </div>
            <div class="summary-item">
                <span>æ–°æ–¹æ³•å¹³å‡åˆ†æ•¸:</span>
                <span class="score new">{results['avg_new_score']:.2f}/100</span>
            </div>
            <div class="summary-item">
                <span>æ•´é«”æ”¹å–„:</span>
                <span class="improvement {'negative' if results['avg_new_score'] - results['avg_old_score'] < 0 else ''}">{results['avg_new_score'] - results['avg_old_score']:+.2f} åˆ†</span>
            </div>
        </div>
        
        <h2>ğŸ“ˆ å„é …æŒ‡æ¨™å¹³å‡æ”¹å–„</h2>
        <div>
            <div class="metric-card">
                <div class="metric-name">å³°å€¼ (Peak)</div>
                <div class="metric-value">{results['avg_improvements']['peak_improvement']:+.2f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-name">RMS èƒ½é‡</div>
                <div class="metric-value">{results['avg_improvements']['rms_improvement']:+.2f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-name">å‹•æ…‹ç¯„åœ</div>
                <div class="metric-value">{results['avg_improvements']['dynamic_range_improvement']:+.2f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-name">æ¸…æ™°åº¦åˆ†æ•¸</div>
                <div class="metric-value">{results['avg_improvements']['clarity_improvement']:+.2f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-name">SNR ä¼°è¨ˆ</div>
                <div class="metric-value">{results['avg_improvements']['snr_improvement']:+.2f}%</div>
            </div>
            <div class="metric-card">
                <div class="metric-name">é«˜é »ä¿ç•™</div>
                <div class="metric-value">{results['avg_improvements']['high_freq_ratio_improvement']:+.2f}%</div>
            </div>
        </div>
        
        <h2>ğŸ“Š è¦–è¦ºåŒ–åœ–è¡¨</h2>
        <div class="chart">
            <img src="{os.path.basename(chart_file)}" alt="è©•ä¼°åœ–è¡¨">
        </div>
        
        <h2>ğŸ“‹ å€‹åˆ¥æª”æ¡ˆè©³ç´°çµæœ</h2>
        <table>
            <thead>
                <tr>
                    <th>æª”æ¡ˆåç¨±</th>
                    <th>èˆŠæ–¹æ³•åˆ†æ•¸</th>
                    <th>æ–°æ–¹æ³•åˆ†æ•¸</th>
                    <th>æ”¹å–„</th>
                </tr>
            </thead>
            <tbody>
"""
        
        for comp in results['comparisons']:
            improvement_class = '' if comp['improvements']['overall_improvement'] >= 0 else 'negative'
            html_content += f"""
                <tr>
                    <td>{comp['filename']}</td>
                    <td>{comp['old_score']:.2f}</td>
                    <td>{comp['new_score']:.2f}</td>
                    <td class="improvement {improvement_class}">{comp['improvements']['overall_improvement']:+.2f}</td>
                </tr>
"""
        
        html_content += """
            </tbody>
        </table>
        
        <div class="footer">
            <p>Generated by Audio Quality Assessment Tool</p>
            <p>ç‰ˆæœ¬ v1.0.0 | 2025-10-25</p>
        </div>
    </div>
</body>
</html>
"""
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        if self.verbose:
            print(f"\nâœ“ HTML å ±å‘Šå·²å„²å­˜è‡³: {output_file}")
            print(f"âœ“ åœ–è¡¨å·²å„²å­˜è‡³: {chart_file}")


def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='éŸ³è¨Šå“è³ªè©•ä¼°å·¥å…·')
    parser.add_argument('--old_dir', type=str, required=True, help='èˆŠæ–¹æ³•è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--new_dir', type=str, required=True, help='æ–°æ–¹æ³•è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--output_txt', type=str, default='quality_report.txt', help='æ–‡å­—å ±å‘Šè¼¸å‡ºè·¯å¾‘')
    parser.add_argument('--output_html', type=str, default='quality_report.html', help='HTML å ±å‘Šè¼¸å‡ºè·¯å¾‘')
    parser.add_argument('--pattern', type=str, default='*.wav', help='æª”æ¡ˆåŒ¹é…æ¨¡å¼')
    parser.add_argument('--sample_rate', type=int, default=16000, help='æ¡æ¨£ç‡')
    
    args = parser.parse_args()
    
    # å‰µå»ºè©•ä¼°å·¥å…·
    print("ğŸµ éŸ³è¨Šå“è³ªè©•ä¼°å·¥å…·")
    print("="*80)
    
    evaluator = AudioQualityAssessment(sample_rate=args.sample_rate, verbose=True)
    
    # åŸ·è¡Œè©•ä¼°
    print(f"\né–‹å§‹å°æ¯”...")
    print(f"èˆŠæ–¹æ³•ç›®éŒ„: {args.old_dir}")
    print(f"æ–°æ–¹æ³•ç›®éŒ„: {args.new_dir}")
    
    results = evaluator.compare_directories(
        old_dir=args.old_dir,
        new_dir=args.new_dir,
        pattern=args.pattern
    )
    
    # ç”Ÿæˆå ±å‘Š
    print(f"\nç”Ÿæˆå ±å‘Š...")
    evaluator.generate_report(results, args.output_txt)
    evaluator.generate_html_report(results, args.output_html)
    
    # é¡¯ç¤ºæ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“Š è©•ä¼°å®Œæˆï¼")
    print("="*80)
    print(f"èˆŠæ–¹æ³•å¹³å‡åˆ†æ•¸: {results['avg_old_score']:.2f}/100")
    print(f"æ–°æ–¹æ³•å¹³å‡åˆ†æ•¸: {results['avg_new_score']:.2f}/100")
    print(f"æ•´é«”æ”¹å–„: {results['avg_new_score'] - results['avg_old_score']:+.2f} åˆ†")
    print("\nä¸»è¦æ”¹å–„:")
    print(f"  â€¢ æ¸…æ™°åº¦: {results['avg_improvements']['clarity_improvement']:+.2f}%")
    print(f"  â€¢ SNR: {results['avg_improvements']['snr_improvement']:+.2f}%")
    print(f"  â€¢ å‹•æ…‹ç¯„åœ: {results['avg_improvements']['dynamic_range_improvement']:+.2f}%")
    print(f"\nè«‹æŸ¥çœ‹è©³ç´°å ±å‘Š:")
    print(f"  â€¢ æ–‡å­—å ±å‘Š: {args.output_txt}")
    print(f"  â€¢ HTML å ±å‘Š: {args.output_html}")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()