#!/usr/bin/env python3
"""
ç­–ç•¥ 2ï¼šæ¢ä»¶å¼é é™å™ªï¼ˆæ™ºæ…§å‹ï¼‰
ç›®æ¨™ï¼šå…ˆè©•ä¼°éŸ³è¨Šå“è³ªï¼Œåªåœ¨å¿…è¦æ™‚æ‰é é™å™ª
é©ç”¨ï¼šæƒ³è¦æœ€ä½³æ•ˆèƒ½å’Œå“è³ªå¹³è¡¡çš„å ´æ™¯
"""

import torch
import torchaudio
import numpy as np
from typing import Tuple, Optional
from dataclasses import dataclass


@dataclass
class AudioQualityMetrics:
    """éŸ³è¨Šå“è³ªæŒ‡æ¨™"""
    snr_db: float           # ä¿¡å™ªæ¯”
    noise_level: float      # å™ªéŸ³æ°´å¹³
    clarity_score: float    # æ¸…æ™°åº¦è©•åˆ†
    need_denoise: bool      # æ˜¯å¦éœ€è¦é™å™ª
    recommended_strength: float  # å»ºè­°é™å™ªå¼·åº¦


class AudioQualityAnalyzer:
    """
    éŸ³è¨Šå“è³ªåˆ†æå™¨
    
    æ±ºç­–é‚è¼¯ï¼š
    1. é«˜å“è³ªï¼ˆSNR > 25 dBï¼‰â†’ ä¸é™å™ª
    2. è‰¯å¥½å“è³ªï¼ˆSNR 15-25 dBï¼‰â†’ è¼•åº¦é™å™ª
    3. ä¸­ç­‰å“è³ªï¼ˆSNR 8-15 dBï¼‰â†’ ä¸­åº¦é™å™ª
    4. ä½å“è³ªï¼ˆSNR < 8 dBï¼‰â†’ è¼ƒå¼·é™å™ª
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sr = sample_rate
        
    def estimate_snr_advanced(self, audio: torch.Tensor) -> float:
        """
        é€²éš SNR ä¼°è¨ˆï¼ˆæ›´æº–ç¢ºï¼‰
        
        æ–¹æ³•ï¼š
        1. VADï¼ˆèªéŸ³æ´»å‹•æª¢æ¸¬ï¼‰æ‰¾å‡ºèªéŸ³æ®µå’ŒéœéŸ³æ®µ
        2. åˆ†åˆ¥è¨ˆç®—èªéŸ³èƒ½é‡å’Œå™ªéŸ³èƒ½é‡
        3. è¨ˆç®— SNR
        """
        frame_length = int(0.025 * self.sr)  # 25ms
        hop_length = int(0.010 * self.sr)    # 10ms
        
        # è¨ˆç®—çŸ­æ™‚èƒ½é‡
        energies = []
        for i in range(0, audio.shape[-1] - frame_length, hop_length):
            frame = audio[..., i:i+frame_length]
            energy = frame.pow(2).mean().item()
            energies.append(energy)
        
        energies = np.array(energies)
        
        if len(energies) == 0:
            return 40.0  # é è¨­é«˜ SNR
        
        # ç°¡å–®çš„ VADï¼šä½¿ç”¨èƒ½é‡é–¾å€¼
        energy_threshold = np.percentile(energies, 40)
        
        speech_frames = energies > energy_threshold
        noise_frames = ~speech_frames
        
        # è¨ˆç®—èªéŸ³å’Œå™ªéŸ³èƒ½é‡
        if speech_frames.sum() > 0 and noise_frames.sum() > 0:
            speech_energy = energies[speech_frames].mean()
            noise_energy = energies[noise_frames].mean()
            
            if noise_energy > 0:
                snr_db = 10 * np.log10(speech_energy / noise_energy)
            else:
                snr_db = 40.0
        else:
            snr_db = 20.0  # é è¨­ä¸­ç­‰ SNR
        
        return snr_db
    
    def estimate_noise_level(self, audio: torch.Tensor) -> float:
        """
        ä¼°è¨ˆå™ªéŸ³æ°´å¹³ï¼ˆ0-1ï¼‰
        
        0 = å¹¾ä¹ç„¡å™ªéŸ³
        1 = å™ªéŸ³å¾ˆå¤§
        """
        # ä½¿ç”¨å‰é¢éœéŸ³æ®µä¼°è¨ˆ
        n_samples = int(0.5 * self.sr)
        n_samples = min(n_samples, audio.shape[-1] // 4)
        
        noise_segment = audio[..., :n_samples]
        noise_level = noise_segment.abs().mean().item()
        
        # æ­£è¦åŒ–åˆ° 0-1
        noise_level = np.clip(noise_level * 10, 0, 1)
        
        return noise_level
    
    def estimate_clarity(self, audio: torch.Tensor) -> float:
        """
        ä¼°è¨ˆæ¸…æ™°åº¦ï¼ˆ0-1ï¼‰
        
        æ–¹æ³•ï¼šçœ‹é »è­œçš„é›†ä¸­ç¨‹åº¦
        - æ¸…æ™°çš„èªéŸ³ï¼šé »è­œé›†ä¸­åœ¨èªéŸ³é »æ®µ
        - æœ‰é›œè¨Šçš„èªéŸ³ï¼šé »è­œåˆ†æ•£
        """
        # è¨ˆç®—é »è­œ
        stft = torch.stft(
            audio,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        
        magnitude = stft.abs()
        power = magnitude.pow(2).mean(dim=-1)  # å¹³å‡åŠŸç‡è­œ
        
        # è¨ˆç®—èªéŸ³é »æ®µï¼ˆ300-3400 Hzï¼‰çš„èƒ½é‡ä½”æ¯”
        freq_bins = torch.fft.rfftfreq(512, 1/self.sr)
        speech_mask = (freq_bins >= 300) & (freq_bins <= 3400)
        
        speech_energy = power[speech_mask].sum()
        total_energy = power.sum()
        
        clarity = (speech_energy / total_energy).item() if total_energy > 0 else 0.5
        
        return clarity
    
    def analyze(self, audio: torch.Tensor) -> AudioQualityMetrics:
        """
        ç¶œåˆåˆ†æéŸ³è¨Šå“è³ª
        """
        # è¨ˆç®—å„é …æŒ‡æ¨™
        snr_db = self.estimate_snr_advanced(audio)
        noise_level = self.estimate_noise_level(audio)
        clarity = self.estimate_clarity(audio)
        
        # æ±ºç­–é‚è¼¯
        if snr_db > 25 and noise_level < 0.1:
            # é«˜å“è³ªï¼šä¸éœ€è¦é™å™ª
            need_denoise = False
            strength = 0.0
        elif snr_db > 15 and noise_level < 0.3:
            # è‰¯å¥½å“è³ªï¼šè¼•åº¦é™å™ª
            need_denoise = True
            strength = 0.2
        elif snr_db > 8:
            # ä¸­ç­‰å“è³ªï¼šä¸­åº¦é™å™ª
            need_denoise = True
            strength = 0.4
        else:
            # ä½å“è³ªï¼šè¼ƒå¼·é™å™ªï¼ˆä½†ä¸è¦å¤ªæ¿€é€²ï¼‰
            need_denoise = True
            strength = 0.6
        
        # æ ¹æ“šæ¸…æ™°åº¦å¾®èª¿
        if clarity < 0.5:
            strength = min(strength + 0.1, 0.7)  # æ¸…æ™°åº¦ä½ï¼Œç¨å¾®åŠ å¼·
        
        return AudioQualityMetrics(
            snr_db=snr_db,
            noise_level=noise_level,
            clarity_score=clarity,
            need_denoise=need_denoise,
            recommended_strength=strength
        )


class ConditionalPreDenoiser:
    """
    æ¢ä»¶å¼é é™å™ªå™¨
    
    æ ¸å¿ƒé‚è¼¯ï¼š
    1. å…ˆåˆ†æéŸ³è¨Šå“è³ª
    2. åªåœ¨å¿…è¦æ™‚æ‰é™å™ª
    3. æ ¹æ“šå“è³ªèª¿æ•´é™å™ªå¼·åº¦
    """
    
    def __init__(
        self,
        sample_rate: int = 16000,
        force_denoise: bool = False,  # å¼·åˆ¶é™å™ªï¼ˆä¸è©•ä¼°å“è³ªï¼‰
        verbose: bool = True
    ):
        self.sr = sample_rate
        self.force_denoise = force_denoise
        self.verbose = verbose
        
        self.analyzer = AudioQualityAnalyzer(sample_rate)
    
    def denoise_with_strength(
        self,
        audio: torch.Tensor,
        strength: float
    ) -> torch.Tensor:
        """
        æ ¹æ“šå¼·åº¦é™å™ª
        
        Args:
            audio: è¼¸å…¥éŸ³è¨Š
            strength: é™å™ªå¼·åº¦ï¼ˆ0-1ï¼‰
        """
        if strength == 0:
            return audio
        
        # ä¼°è¨ˆå™ªéŸ³ç‰¹å¾µ
        n_samples = int(0.5 * self.sr)
        n_samples = min(n_samples, audio.shape[-1] // 4)
        noise_segment = audio[..., :n_samples]
        
        # STFT
        stft_noise = torch.stft(
            noise_segment,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        noise_power = stft_noise.abs().pow(2).mean(dim=-1, keepdim=True)
        
        stft = torch.stft(
            audio,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            return_complex=True
        )
        
        magnitude = stft.abs()
        phase = stft.angle()
        signal_power = magnitude.pow(2)
        
        # è¨ˆç®—å¢ç›Šï¼ˆWiener-like filteringï¼‰
        snr = signal_power / (noise_power + 1e-10)
        gain = snr / (snr + 1)
        
        # æ ¹æ“š strength èª¿æ•´
        gain = 1 - (1 - gain) * strength
        
        # å¹³æ»‘å¢ç›Šï¼ˆæ™‚é–“è»¸ï¼‰
        kernel = torch.ones(1, 1, 5, device=audio.device) / 5
        gain = gain.unsqueeze(0).unsqueeze(0)
        gain = torch.nn.functional.conv2d(
            gain, kernel, padding=(0, 2)
        ).squeeze(0).squeeze(0)
        
        # æ‡‰ç”¨
        filtered_magnitude = magnitude * gain
        filtered_stft = filtered_magnitude * torch.exp(1j * phase)
        
        # ISTFT
        denoised = torch.istft(
            filtered_stft,
            n_fft=512,
            hop_length=128,
            window=torch.hann_window(512, device=audio.device),
            length=audio.shape[-1]
        )
        
        return torch.clamp(denoised, -1.0, 1.0)
    
    def __call__(
        self,
        audio: torch.Tensor
    ) -> Tuple[torch.Tensor, AudioQualityMetrics]:
        """
        æ¢ä»¶å¼é é™å™ª
        
        Returns:
            denoised: è™•ç†å¾Œçš„éŸ³è¨Š
            metrics: å“è³ªåˆ†æçµæœ
        """
        original_shape = audio.shape
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
        
        # åˆ†æå“è³ª
        metrics = self.analyzer.analyze(audio)
        
        if self.verbose:
            print(f"\n{'='*60}")
            print("éŸ³è¨Šå“è³ªåˆ†æ")
            print(f"{'='*60}")
            print(f"SNR: {metrics.snr_db:.2f} dB")
            print(f"å™ªéŸ³æ°´å¹³: {metrics.noise_level:.3f}")
            print(f"æ¸…æ™°åº¦: {metrics.clarity_score:.3f}")
            print(f"å»ºè­°é™å™ª: {'æ˜¯' if metrics.need_denoise else 'å¦'}")
            if metrics.need_denoise:
                print(f"å»ºè­°å¼·åº¦: {metrics.recommended_strength*100:.1f}%")
            print(f"{'='*60}\n")
        
        # æ±ºå®šæ˜¯å¦é™å™ª
        if self.force_denoise or metrics.need_denoise:
            strength = metrics.recommended_strength if not self.force_denoise else 0.4
            denoised = self.denoise_with_strength(audio, strength)
            
            if self.verbose:
                print(f"âœ“ å·²åŸ·è¡Œé é™å™ªï¼ˆå¼·åº¦: {strength*100:.1f}%ï¼‰")
        else:
            denoised = audio
            
            if self.verbose:
                print("âœ“ éŸ³è¨Šå“è³ªè‰¯å¥½ï¼Œè·³éé é™å™ª")
        
        if len(original_shape) == 1:
            denoised = denoised.squeeze(0)
        
        return denoised, metrics


# ============================================================
# å®Œæ•´æµç¨‹æ•´åˆ
# ============================================================

class IntelligentPipeline:
    """
    æ™ºæ…§å‹è™•ç†æµç¨‹
    
    ç‰¹é»ï¼š
    1. è‡ªå‹•è©•ä¼°æ˜¯å¦éœ€è¦é é™å™ª
    2. æ ¹æ“šå“è³ªèª¿æ•´æ¯å€‹æ­¥é©Ÿçš„åƒæ•¸
    3. æœ€ä½³åŒ–æ•ˆèƒ½å’Œå“è³ª
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sr = sample_rate
        self.pre_denoiser = ConditionalPreDenoiser(sample_rate, verbose=True)
    
    def process(self, audio_path: str):
        """
        å®Œæ•´è™•ç†æµç¨‹
        """
        print(f"\n{'='*60}")
        print(f"æ™ºæ…§å‹è™•ç†æµç¨‹")
        print(f"è¼¸å…¥ï¼š{audio_path}")
        print(f"{'='*60}\n")
        
        # 1. è¼‰å…¥éŸ³è¨Š
        print("æ­¥é©Ÿ 1/5ï¼šè¼‰å…¥éŸ³è¨Š...")
        audio, sr = torchaudio.load(audio_path)
        if sr != self.sr:
            resampler = torchaudio.transforms.Resample(sr, self.sr)
            audio = resampler(audio)
        print(f"âœ“ æ¡æ¨£ç‡ï¼š{self.sr} Hzï¼Œé•·åº¦ï¼š{audio.shape[-1]/self.sr:.2f} ç§’\n")
        
        # 2. æ¢ä»¶å¼é é™å™ª
        print("æ­¥é©Ÿ 2/5ï¼šæ¢ä»¶å¼é é™å™ª...")
        audio_processed, metrics = self.pre_denoiser(audio)
        
        # æ ¹æ“šå“è³ªæ±ºå®šå¾ŒçºŒåƒæ•¸
        if metrics.snr_db < 10:
            print("âš ï¸  è¼¸å…¥å“è³ªè¼ƒä½ï¼Œå°‡ä½¿ç”¨æ›´é­¯æ£’çš„åƒæ•¸\n")
            # å¯ä»¥èª¿æ•´å¾ŒçºŒæ¨¡å‹çš„åƒæ•¸
        
        # å„²å­˜é é™å™ªçµæœï¼ˆç”¨æ–¼å¾ŒçºŒæ­¥é©Ÿï¼‰
        torchaudio.save("temp_pre_denoised.wav", audio_processed.unsqueeze(0), self.sr)
        
        # 3. èªªè©±è€…åˆ¤æ–·
        print("æ­¥é©Ÿ 3/5ï¼šèªªè©±è€…åˆ¤æ–·...")
        # ä½¿ç”¨é é™å™ªå¾Œçš„éŸ³è¨Šï¼ˆå¦‚æœæœ‰é™å™ªçš„è©±ï¼‰
        # num_speakers = your_diarization(audio_processed)
        print("âœ“ æª¢æ¸¬åˆ° X ä½èªªè©±è€…\n")
        
        # 4. èªéŸ³åˆ†é›¢
        print("æ­¥é©Ÿ 4/5ï¼šèªéŸ³åˆ†é›¢...")
        # separated = your_separation(audio_processed)
        print("âœ“ åˆ†é›¢å®Œæˆ\n")
        
        # 5. å¾Œè™•ç†é™å™ª
        print("æ­¥é©Ÿ 5/5ï¼šå¾Œè™•ç†é™å™ª...")
        # æ ¹æ“šé é™å™ªçš„å“è³ªæ±ºå®šå¾Œé™å™ªå¼·åº¦
        if metrics.need_denoise:
            print("   ä½¿ç”¨è¼ƒå¼·çš„å¾Œé™å™ª")
            post_denoise_strength = 0.8
        else:
            print("   ä½¿ç”¨æ¨™æº–å¾Œé™å™ª")
            post_denoise_strength = 0.6
        
        # final = post_denoise(separated, strength=post_denoise_strength)
        print("âœ“ å¾Œè™•ç†å®Œæˆ\n")
        
        print(f"{'='*60}")
        print("è™•ç†å®Œæˆï¼")
        print(f"{'='*60}\n")
        
        return {
            'metrics': metrics,
            'pre_denoised': audio_processed,
            # 'separated': separated,
            # 'final': final
        }


def benchmark_pre_denoise_effect(audio_path: str):
    """
    æ¸¬è©¦é é™å™ªçš„å¯¦éš›æ•ˆæœ
    
    æ¯”è¼ƒï¼š
    1. ç„¡é é™å™ª
    2. æœ‰é é™å™ª
    
    æŒ‡æ¨™ï¼š
    - èªªè©±è€…åˆ¤æ–·æº–ç¢ºåº¦
    - åˆ†é›¢å“è³ª
    - è™•ç†æ™‚é–“
    """
    import time
    
    print(f"\n{'='*70}")
    print("é é™å™ªæ•ˆæœæ¸¬è©¦")
    print(f"{'='*70}\n")
    
    audio, sr = torchaudio.load(audio_path)
    
    # æ¸¬è©¦ 1ï¼šç„¡é é™å™ª
    print("ğŸ”¹ æ¸¬è©¦ 1ï¼šç„¡é é™å™ª")
    print("-" * 70)
    
    start = time.time()
    
    # èªªè©±è€…åˆ¤æ–·
    # result_1 = your_pipeline(audio)
    
    time_1 = time.time() - start
    
    print(f"è™•ç†æ™‚é–“ï¼š{time_1:.2f} ç§’")
    # print(f"æª¢æ¸¬èªªè©±è€…ï¼š{result_1['num_speakers']} ä½")
    # print(f"åˆ†é›¢å“è³ªï¼š{result_1['quality_score']:.2f}")
    print()
    
    # æ¸¬è©¦ 2ï¼šæœ‰é é™å™ª
    print("ğŸ”¹ æ¸¬è©¦ 2ï¼šæœ‰é é™å™ª")
    print("-" * 70)
    
    denoiser = ConditionalPreDenoiser(sample_rate=sr, verbose=True)
    audio_denoised, metrics = denoiser(audio)
    
    start = time.time()
    
    # èªªè©±è€…åˆ¤æ–·
    # result_2 = your_pipeline(audio_denoised)
    
    time_2 = time.time() - start
    
    print(f"è™•ç†æ™‚é–“ï¼š{time_2:.2f} ç§’ï¼ˆä¸å«é é™å™ªï¼‰")
    # print(f"æª¢æ¸¬èªªè©±è€…ï¼š{result_2['num_speakers']} ä½")
    # print(f"åˆ†é›¢å“è³ªï¼š{result_2['quality_score']:.2f}")
    print()
    
    # çµæœæ¯”è¼ƒ
    print(f"{'='*70}")
    print("çµæœæ¯”è¼ƒ")
    print(f"{'='*70}")
    print(f"ç¸½è™•ç†æ™‚é–“ï¼š{time_1:.2f}s (ç„¡é é™å™ª) vs {time_2:.2f}s (æœ‰é é™å™ª)")
    # print(f"èªªè©±è€…æª¢æ¸¬ï¼š{result_1['num_speakers']} vs {result_2['num_speakers']}")
    # print(f"åˆ†é›¢å“è³ªï¼š{result_1['quality_score']:.2f} vs {result_2['quality_score']:.2f}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    # ç¯„ä¾‹ 1ï¼šæ¢ä»¶å¼é é™å™ª
    print("ç¯„ä¾‹ 1ï¼šæ¢ä»¶å¼é é™å™ª")
    print("=" * 60)
    
    denoiser = ConditionalPreDenoiser(sample_rate=16000, verbose=True)
    
    audio, sr = torchaudio.load("speaker1.wav")
    denoised, metrics = denoiser(audio)
    
    torchaudio.save("speaker1_conditional_denoised.wav", denoised.unsqueeze(0), sr)
    print("\nâœ“ å·²å„²å­˜ï¼šspeaker1_conditional_denoised.wav\n")
    
    # ç¯„ä¾‹ 2ï¼šæ™ºæ…§å‹å®Œæ•´æµç¨‹
    # pipeline = IntelligentPipeline(sample_rate=16000)
    # result = pipeline.process("your_audio.wav")
    
    # ç¯„ä¾‹ 3ï¼šæ•ˆæœæ¸¬è©¦
    # benchmark_pre_denoise_effect("your_audio.wav")